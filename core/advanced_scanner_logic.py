# -*- coding: utf-8 -*-

# plik: core/advanced_scanner_logic.py
# Wersja 6.0 - W pe≈Çni asynchroniczny, udokumentowany i zintegrowany z nowym modu≈Çem bazy danych
#
# ##############################################################################
# ===                        JAK TO DZIA≈ÅA (PROSTE WYJA≈öNIENIE)                ===
# ##############################################################################
#
# Ten plik zawiera logikƒô zaawansowanego, wielofunkcyjnego narzƒôdzia do
# zarzƒÖdzania metadanymi i sp√≥jno≈õciƒÖ kolekcji. Pe≈Çni trzy g≈Ç√≥wne role:
#
#  1. SKANER ONLINE: Pobiera bogate metadane (opisy, albumy, tagi, GPS)
#     bezpo≈õrednio ze strony Google Photos i oblicza OCZEKIWANƒÑ, idealnƒÖ
#     lokalizacjƒô pliku (`expected_path`) na dysku.
#
#  2. KOREKTOR OFFLINE: Por√≥wnuje rzeczywistƒÖ lokalizacjƒô pobranych plik√≥w
#     z ich oczekiwanƒÖ lokalizacjƒÖ i pozwala na ich automatycznƒÖ naprawƒô.
#
#  3. ZAPISYWARKA EXIF: Odczytuje metadane z bazy danych i zapisuje je
#     bezpo≈õrednio w plikach na dysku.
#
################################################################################

# --- G≈Å√ìWNE IMPORTY ---
import asyncio
import json
import re
import shutil
import logging
from collections import deque
from datetime import datetime
from pathlib import Path
import aiosqlite

# --- Zale≈ºno≈õci zewnƒôtrzne (opcjonalne) ---
try:
    import exiftool
    EXIFTOOL_AVAILABLE = True
except ImportError:
    EXIFTOOL_AVAILABLE = False
    
# --- Playwright ---
from playwright.async_api import async_playwright, Page

# --- Importy z `rich` ---
from rich.console import Console, Group
from rich.panel import Panel
from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn
from rich.live import Live
from rich.layout import Layout
from rich.table import Table
from rich.text import Text
from rich.prompt import Confirm, Prompt

# --- IMPORTY Z W≈ÅASNYCH MODU≈Å√ìW `core` ---
# Jawne importy z pliku konfiguracyjnego
from .config import (
    DATABASE_FILE, SESSION_DIR, DOWNLOADS_DIR_BASE, URL_INPUT_FILE,
    WAIT_FOR_SELECTOR, WAIT_FOR_PAGE_LOAD, BROWSER_TYPE, BROWSER_ARGS,
    ENABLE_RESOURCE_BLOCKING, INFO_PANEL_BUTTON_SELECTOR,
    DEFAULT_HEADLESS_MODE, BLOCKED_RESOURCE_TYPES
)

# Importujemy nowe, asynchroniczne funkcje z naszego modu≈Çu bazy danych
from .database import (
    setup_database,
    get_urls_for_online_scan, # <-- NOWA, DEDYKOWANA FUNKCJA
    update_scanned_entries_batch, # <-- NOWA, DEDYKOWANA FUNKCJA
    get_urls_to_fix, # <-- NOWA, DEDYKOWANA FUNKCJA
    get_all_urls_from_db # <-- NOWA, DEDYKOWANA FUNKCJA
)

from .utils import stop_event, get_date_from_metadata, create_unique_filepath, create_interactive_menu
from .config_editor_logic import get_key

# --- INICJALIZACJA I KONFIGURACJA MODU≈ÅU ---
console = Console(record=True)
logger = logging.getLogger(__name__)

# Definicje sta≈Çych dla plik√≥w log√≥w i postƒôpu
LOG_FILE = Path("app_data/dziennik/advanced_scanner.log")
BATCH_SIZE = 50 # Liczba wynik√≥w zapisywanych do bazy w jednej transakcji


# ##############################################################################
# ===            SEKCJA 1: FUNKCJE POMOCNICZE I NARZƒòDZIA MENU               ===
# ##############################################################################


async def export_fix_needed_urls_to_file():
    """
    Eksportuje do pliku `urls_to_fix.txt` listƒô adres√≥w URL, kt√≥re wymagajƒÖ
    ponownego skanowania w celu uzupe≈Çnienia brakujƒÖcych metadanych.

    Funkcja ta wywo≈Çuje dedykowanƒÖ metodƒô z modu≈Çu bazy danych, aby znale≈∫ƒá
    wpisy, kt√≥re majƒÖ ju≈º jakie≈õ metadane, ale brakuje im kluczowych p√≥l,
    takich jak `FileName`. Jest to przydatne do naprawy czƒô≈õciowo
    przetworzonych kolekcji.
    """
    FIX_URL_FILE = "urls_to_fix.txt"
    console.clear()
    logger.info(f"Rozpoczynam eksport URL-i wymagajƒÖcych naprawy do pliku '{FIX_URL_FILE}'...")
    console.print(Panel(f"üì¶ Eksport URL-i do Naprawy do Pliku '{FIX_URL_FILE}'", expand=False, style="blue"))

    try:
        # Krok 1: Wywo≈Çaj asynchronicznƒÖ funkcjƒô z modu≈Çu bazy danych
        urls_to_fix = await get_urls_to_fix()

        if not urls_to_fix:
            logger.info("Nie znaleziono ≈ºadnych wpis√≥w wymagajƒÖcych naprawy metadanych.")
            console.print("\n[bold green]‚úÖ WyglƒÖda na to, ≈ºe wszystkie metadany w bazie sƒÖ kompletne.[/bold green]")
            return

        # Krok 2: Zapisz znalezione URL-e do pliku
        output_file = Path(FIX_URL_FILE)
        with open(output_file, "w", encoding="utf-8") as f:
            for url in urls_to_fix:
                f.write(f"{url}\n")
        
        logger.info(f"Sukces! Wyeksportowano {len(urls_to_fix)} adres√≥w URL do pliku.")
        console.print(f"\n[bold green]‚úÖ Pomy≈õlnie zapisano {len(urls_to_fix)} URL-i w pliku:[/bold green]")
        console.print(f"[cyan]{output_file.resolve()}[/cyan]")

    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas eksportu URL-i do naprawy: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd. Sprawd≈∫ plik logu, aby uzyskaƒá wiƒôcej informacji.[/bold red]")


async def block_unwanted_resources(route):
    """
    Przechwytuje i opcjonalnie blokuje ≈ºƒÖdania sieciowe strony.

    Ta funkcja jest podpinana jako "handler" do przeglƒÖdarki i wywo≈Çywana
    dla ka≈ºdego pojedynczego ≈ºƒÖdania (o obrazek, styl, czcionkƒô itp.).
    Jest to kluczowa optymalizacja, kt√≥ra znaczƒÖco przyspiesza ≈Çadowanie
    stron i zmniejsza zu≈ºycie transferu danych poprzez ignorowanie
    niepotrzebnych zasob√≥w zdefiniowanych w `BLOCKED_RESOURCE_TYPES`.

    Args:
        route: Obiekt Playwright reprezentujƒÖcy pojedyncze ≈ºƒÖdanie sieciowe.
    """
    resource_type = route.request.resource_type
    if resource_type in BLOCKED_RESOURCE_TYPES:
        logger.debug(f"Blokujƒô zas√≥b typu '{resource_type}': {route.request.url[:80]}...")
        await route.abort()
    else:
        await route.continue_()


async def export_urls_from_db_to_file():
    """
    Eksportuje wszystkie adresy URL z tabeli `downloaded_media` w bazie
    danych do pliku tekstowego zdefiniowanego w `config.py`.

    Jest to przydatne narzƒôdzie do stworzenia listy startowej dla trybu
    skanowania "Skanuj z pliku" (`scan_all`).
    """
    console.clear()
    logger.info(f"Rozpoczynam eksport wszystkich adres√≥w URL do pliku '[bold cyan]{URL_INPUT_FILE}[/bold cyan]'...", extra={"markup": True})
    console.print(Panel(f"üì¶ Eksport Wszystkich URL-i z Bazy do Pliku", expand=False, style="blue"))

    try:
        # Krok 1: Wywo≈Çaj asynchronicznƒÖ funkcjƒô z modu≈Çu bazy danych
        urls = await get_all_urls_from_db()
        
        if not urls:
            logger.warning("Baza danych jest pusta lub nie zawiera ≈ºadnych adres√≥w URL.")
            console.print("\n[bold yellow]Nie znaleziono ≈ºadnych adres√≥w URL w bazie danych do wyeksportowania.[/bold yellow]")
            return
            
        # Krok 2: Zapisz znalezione URL-e do pliku
        output_file = Path(URL_INPUT_FILE)
        with open(output_file, "w", encoding="utf-8") as f:
            for url in urls:
                f.write(f"{url}\n")
                
        logger.info(f"Sukces! Wyeksportowano {len(urls)} adres√≥w URL do pliku.")
        console.print(f"\n[bold green]‚úÖ Pomy≈õlnie zapisano {len(urls)} URL-i w pliku:[/bold green]")
        console.print(f"[cyan]{output_file.resolve()}[/cyan]")
        
    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas eksportu adres√≥w URL: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd. Sprawd≈∫ plik logu, aby uzyskaƒá wiƒôcej informacji.[/bold red]")


async def get_urls_for_processing(process_mode: str, input_file: str = URL_INPUT_FILE) -> list[str] | None:
    """
    Przygotowuje listƒô adres√≥w URL do przetworzenia w zale≈ºno≈õci od trybu pracy.

    Pe≈Çni rolƒô "dyspozytora", kt√≥ry decyduje, skƒÖd pobraƒá listƒô URL-i:
    - Dla tryb√≥w 'scan_all' i 'scan_fix_file', wczytuje URL-e z podanego pliku.
    - Dla tryb√≥w opartych na bazie danych ('full_scan', 'retry_errors',
      'force_refresh'), wywo≈Çuje odpowiedniƒÖ, asynchronicznƒÖ funkcjƒô z modu≈Çu
      `database.py`, kt√≥ra zwraca przefiltrowanƒÖ listƒô.

    Args:
        process_mode (str): Tryb pracy skanera ('full_scan', 'scan_all', etc.).
        input_file (str): ≈öcie≈ºka do pliku wej≈õciowego (u≈ºywana tylko w trybach
                          skanowania z pliku).

    Returns:
        list[str] | None: Lista adres√≥w URL do przetworzenia. Zwraca None w
                          przypadku krytycznego b≈Çƒôdu odczytu pliku.
    """
    logger.info(f"Przygotowujƒô listƒô URL-i do przetworzenia w trybie: [bold]{process_mode}[/bold]", extra={"markup": True})

    # --- Tryby odczytu z pliku ---
    if process_mode in ['scan_all', 'scan_fix_file']:
        try:
            url_file = Path(input_file)
            if not url_file.exists():
                logger.warning(f"Nie znaleziono pliku wej≈õciowego '{url_file}'.")
                console.print(f"\n[bold yellow]Plik '{url_file}' nie istnieje. U≈ºyj opcji eksportu w menu, aby go utworzyƒá.[/bold yellow]")
                return []
            with open(url_file, "r", encoding="utf-8") as f:
                urls = [line.strip() for line in f if line.strip().startswith("http")]
            if not urls:
                logger.warning(f"Plik '{url_file}' jest pusty lub nie zawiera prawid≈Çowych link√≥w.")
            else:
                logger.info(f"Znaleziono {len(urls)} URL-i w pliku '{url_file.name}'.")
            return urls
        except Exception as e:
            logger.critical(f"B≈ÅƒÑD: Nie mo≈ºna odczytaƒá pliku '{url_file}': {e}", exc_info=True)
            return None

    # --- Tryby odczytu z bazy danych ---
    try:
        # Mapowanie trybu z menu na tryb dla funkcji z modu≈Çu bazy danych
        scan_type_map = {
            'retry_errors': 'retry_errors',
            'force_refresh': 'force_refresh',
            'full_scan': 'new_only' # 'full_scan' w tym module oznacza skanowanie tylko nowych
        }
        scan_type = scan_type_map.get(process_mode, 'new_only')
        
        urls_from_db = await get_urls_for_online_scan(scan_type)
        logger.info(f"Znaleziono {len(urls_from_db)} URL-i w bazie danych pasujƒÖcych do kryteri√≥w.")
        return urls_from_db
    except Exception as e:
        logger.critical(f"B≈ÅƒÑD: Nie mo≈ºna pobraƒá danych z bazy: {e}", exc_info=True)
        return None


def log_to_file(url: str, details: dict, status: str):
    """
    Zapisuje szczeg√≥≈Çowy log pojedynczej operacji do pliku tekstowego.

    Jest to dodatkowy mechanizm logowania, niezale≈ºny od g≈Ç√≥wnego systemu
    `logging`, przeznaczony do tworzenia czytelnego raportu z przebiegu
    skanowania metadanych. Ka≈ºdy wpis zawiera datƒô, URL, status
    oraz pe≈Çny zrzut zebranych metadanych w formacie JSON.

    Args:
        url (str): Przetwarzany adres URL.
        details (dict): S≈Çownik z zebranymi metadanymi.
        status (str): Status operacji (np. 'Sukces', 'B≈ÇƒÖd').
    """
    try:
        log_path = Path(LOG_FILE)
        # Upewnij siƒô, ≈ºe folder na logi istnieje
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"--- {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
            f.write(f"URL: {url}\n")
            f.write(f"Status: {status}\n")
            if details:
                # U≈ºywamy json.dumps do ≈Çadnego sformatowania s≈Çownika
                f.write(json.dumps(details, ensure_ascii=False, indent=4))
            f.write("\n\n")
            
        logger.debug(f"Zapisano wpis dla URL ...{url[-40:]} do pliku logu '{log_path.name}'.")

    except Exception as e:
        # U≈ºywamy g≈Ç√≥wnego loggera, aby zarejestrowaƒá problem z zapisem do pliku logu
        logger.error(f"Nie uda≈Ço siƒô zapisaƒá do pliku logu '{LOG_FILE}': {e}", exc_info=True)


# ##############################################################################
# ===            SEKCJA 2: G≈Å√ìWNA LOGIKA SKANERA I KOREKTOR√ìW                ===
# ##############################################################################

async def get_advanced_photo_details_from_page(page: Page, current_url: str) -> dict | None:
    """
    Skaner Online: Pobiera wszystkie zaawansowane metadane ze strony zdjƒôcia
    i oblicza OCZEKIWANƒÑ ≈õcie≈ºkƒô zapisu (`expected_path`).

    Proces:
    1.  Otwiera panel boczny z informacjami, kt√≥ry zawiera wiƒôkszo≈õƒá metadanych.
    2.  Pr√≥buje wyodrƒôbniƒá datƒô z dw√≥ch niezale≈ºnych ≈∫r√≥de≈Ç dla wiƒôkszej
        niezawodno≈õci:
        a) Z tekstu w panelu bocznym (g≈Ç√≥wne ≈∫r√≥d≈Ço).
        b) Z atrybutu `aria-label` g≈Ç√≥wnego obrazka (≈∫r√≥d≈Ço zapasowe).
    3.  Pobiera pozosta≈Çe metadane (nazwa pliku, aparat, lokalizacja, opis,
        osoby, albumy) za pomocƒÖ precyzyjnych selektor√≥w CSS.
    4.  W przypadku braku kluczowej daty, inteligentnie od≈õwie≈ºa stronƒô i
        ponawia pr√≥bƒô skanowania.
    5.  Na podstawie zebranych danych oblicza `expected_path`.
    6.  Dodatkowo, zbiera dane z selektor√≥w eksperymentalnych do cel√≥w
        diagnostycznych.

    Args:
        page (Page): Obiekt strony Playwright.
        current_url (str): URL aktualnie analizowanej strony.

    Returns:
        dict | None: S≈Çownik z zebranymi metadanymi lub None w przypadku b≈Çƒôdu.
    """
    months_map = {
        'sty': 1, 'lut': 2, 'mar': 3, 'kwi': 4, 'maj': 5, 'cze': 6,
        'lip': 7, 'sie': 8, 'wrz': 9, 'pa≈∫': 10, 'lis': 11, 'gru': 12
    }
    
    async def get_attribute_safely(locator, attribute='aria-label'):
        """Bezpiecznie pobiera atrybut, aby uniknƒÖƒá b≈Çƒôd√≥w."""
        try:
            return await locator.get_attribute(attribute, timeout=1000)
        except Exception:
            return None

    async def _scan_page_content():
        """Wykonuje pojedynczƒÖ pr√≥bƒô skanowania zawarto≈õci strony."""
        logger.debug(f"Rozpoczynam skanowanie zawarto≈õci strony dla URL: ...{current_url[-40:]}")
        scan_results = {}
        
        photo_id_match = re.search(r'AF1Qip[\w-]+', current_url)
        if not photo_id_match:
            logger.warning(f"Nie uda≈Ço siƒô wyodrƒôbniƒá ID zdjƒôcia z URL: {current_url}")
            return None
        photo_id = photo_id_match.group(0)

        # Krok 1: Otw√≥rz panel boczny (je≈õli jest zamkniƒôty)
        info_panel_selector = f"c-wiz[jslog*='{photo_id}']"
        try:
            if not await page.is_visible(info_panel_selector):
                await page.click(INFO_PANEL_BUTTON_SELECTOR, timeout=WAIT_FOR_SELECTOR * 1000)
                await page.wait_for_selector(info_panel_selector, timeout=WAIT_FOR_SELECTOR * 1000, state="visible")
                logger.debug("Panel boczny zosta≈Ç pomy≈õlnie otwarty.")
        except Exception:
            logger.warning("Nie uda≈Ço siƒô otworzyƒá panelu bocznego. Metadane mogƒÖ byƒá niekompletne.")

        wiz_element = page.locator(info_panel_selector).first

        # Krok 2: Ekstrakcja stabilnych danych
        
        # Data z panelu bocznego i atrybutu aria-label
        date_from_panel, date_from_aria = None, None
        # ... (ta logika pozostaje bez zmian) ...
        date_text_locator = wiz_element.locator(".R9U8ab")
        if await date_text_locator.count() > 0:
            date_text = await date_text_locator.first.inner_text()
            match = re.search(r'(\d{1,2})\s+([a-zA-Z]{3})\s+(\d{4}),\s+(\d{2}:\d{2})', date_text)
            if match:
                day, month_str, year, time_str = match.groups()
                if month := months_map.get(month_str.lower()):
                    hour, minute = map(int, time_str.split(':'))
                    date_from_panel = datetime(int(year), month, int(day), hour, minute)
                    scan_results["DateTime_Panel"] = date_from_panel.isoformat()
                    logger.debug(f"Znaleziono datƒô w panelu bocznym: {date_from_panel.isoformat()}")

        main_image_locator = page.locator("img.BiCYpc[aria-label]")
        if await main_image_locator.count() > 0:
            aria_label = await main_image_locator.first.get_attribute('aria-label')
            if aria_label:
                match_aria = re.search(r'(\d{1,2})\s+([a-zA-Z]{3})\s+(\d{4}),\s+(\d{2}:\d{2}:\d{2})', aria_label, re.IGNORECASE)
                if match_aria:
                    day, month_str, year, time_str = match_aria.groups()
                    if month := months_map.get(month_str.lower()):
                        h, m, s = map(int, time_str.split(':'))
                        date_from_aria = datetime(int(year), month, int(day), h, m, s)
                        scan_results["DateTime_AriaLabel"] = date_from_aria.isoformat()
                        logger.debug(f"Znaleziono datƒô w atrybucie aria-label: {date_from_aria.isoformat()}")

        if date_from_panel:
            scan_results["DateTime"] = date_from_panel.isoformat()
            scan_results["DateTimeSource"] = "Panel Boczny"
        elif date_from_aria:
            scan_results["DateTime"] = date_from_aria.isoformat()
            scan_results["DateTimeSource"] = "Atrybut 'aria-label'"
        
        # Pobieranie pozosta≈Çych metadanych
        pairs = {
            "FileName": "div.R9U8ab[aria-label^='Nazwa pliku:']",
            "Camera": "div.R9U8ab[aria-label^='Nazwa aparatu:']",
            "Location": "div.R9U8ab[aria-label='Lokalizacja']",
            "Dimensions": "span[aria-label^='Rozmiar w pikselach']",
            "FileSize": "span[aria-label^='Rozmiar pliku:']"
        }
        for key, selector in pairs.items():
            locator = wiz_element.locator(selector).first
            if await locator.count() > 0:
                if aria := await get_attribute_safely(locator):
                    value = aria.split(":", 1)[-1].strip() if ':' in aria else aria
                    scan_results[key] = value
                    logger.debug(f"Znaleziono '{key}': {value}")
        
        if description := await wiz_element.locator("textarea[aria-label='Opis']").input_value():
            scan_results["Description"] = description.strip()
            logger.debug(f"Znaleziono opis: '{description[:50]}...'")
        
        people_locators = await wiz_element.locator("a[aria-label^='Na zdjƒôciu:']").all()
        if tagged_people := [await get_attribute_safely(loc) for loc in people_locators]:
            scan_results["TaggedPeople"] = [p.replace("Na zdjƒôciu:", "").strip() for p in tagged_people if p]
            logger.debug(f"Znaleziono osoby: {scan_results['TaggedPeople']}")
        
        albums_section = wiz_element.locator("div.KlIBpb:has-text('Albumy')")
        if await albums_section.count() > 0:
            album_locators = await albums_section.locator("div.AJM7gb").all()
            scan_results["Albums"] = [await loc.inner_text() for loc in album_locators]
            logger.debug(f"Znaleziono albumy: {scan_results['Albums']}")

        # --- Krok 3: Ekstrakcja Danych Eksperymentalnych/Testowych ---
        # Ta sekcja zawiera selektory, kt√≥re sƒÖ mniej stabilne lub s≈Çu≈ºƒÖ
        # do cel√≥w diagnostycznych. Dane z nich sƒÖ dodawane do pod-s≈Çownika
        # 'Experimental_Details', aby nie mieszaƒá ich z g≈Ç√≥wnymi metadanymi.
        
        experimental_details = {}
        
        map_link_locator = wiz_element.locator("a.cFLCHe")
        if await map_link_locator.count() > 0:
            href = await map_link_locator.get_attribute('href')
            if href and (gps_match := re.search(r'(-?\d+\.\d+),(-?\d+\.\d+)', href)):
                experimental_details["GPS_Coords"] = {"latitude": float(gps_match.group(1)), "longitude": float(gps_match.group(2))}
        
        upload_source_locator = wiz_element.locator("div.ffq9nc:has-text('Przes≈Çane z') dd.rCexAf")
        if await upload_source_locator.count() > 0:
            experimental_details["Upload_Source"] = await upload_source_locator.first.inner_text()
            
        album_info_locator = albums_section.locator(".rugHuc").first
        if await album_info_locator.count() > 0:
            album_text = await album_info_locator.inner_text()
            if match := re.search(r'(\d+)\s+element', album_text):
                experimental_details["Album_Element_Count"] = int(match.group(1))

        if experimental_details:
            scan_results["Experimental_Details"] = experimental_details
            logger.debug(f"Zebrano {len(experimental_details)} dodatkowych danych eksperymentalnych.")
            
        return scan_results
    
    # --- G≈Ç√≥wna logika wykonania z mechanizmem ponawiania ---
    try:
        details = await _scan_page_content()

        if details is not None and "DateTime" not in details:
            logger.warning(f"Brak kluczowej daty dla ...{current_url[-40:]}. Od≈õwie≈ºam i pr√≥bujƒô ponownie...", extra={"markup": True})
            await page.reload(wait_until='load', timeout=WAIT_FOR_PAGE_LOAD * 1000)
            await asyncio.sleep(2)
            details = await _scan_page_content()

        if details is None:
            return None

        # Oblicz OCZEKIWANƒÑ ≈õcie≈ºkƒô zapisu na podstawie zebranych metadanych
        if "DateTime" in details and "FileName" in details:
            try:
                dt = datetime.fromisoformat(details["DateTime"])
                dest_dir = Path(DOWNLOADS_DIR_BASE) / str(dt.year) / f"{dt.month:02d}"
                details['expected_path'] = str(dest_dir / details['FileName'])
                logger.debug(f"Obliczono oczekiwanƒÖ ≈õcie≈ºkƒô: {details['expected_path']}")
            except (ValueError, TypeError):
                details['expected_path'] = None
        else:
            details['expected_path'] = None
        
        logger.info(f"Skanowanie online dla ...{current_url[-40:]} zako≈Ñczone pomy≈õlnie.")
        return details

    except Exception as e:
        logger.error(f"Krytyczny b≈ÇƒÖd podczas analizy strony {current_url}: {e}", exc_info=True)
        return None


async def run_scanner_core(process_mode: str, run_headless: bool, input_file: str = URL_INPUT_FILE):
    """
    G≈Ç√≥wna pƒôtla wykonawcza dla skanera dzia≈ÇajƒÖcego w trybie online.

    Odpowiada za:
    - Inicjalizacjƒô przeglƒÖdarki i interfejsu u≈ºytkownika Rich.
    - Pobranie listy URL-i do przetworzenia za pomocƒÖ `get_urls_for_processing`.
    - Iterowanie po li≈õcie URL-i.
    - Wywo≈Çywanie `get_advanced_photo_details_from_page` dla ka≈ºdego URL-a.
    - Obs≈Çugƒô logiki ponawiania pr√≥b w przypadku b≈Çƒôd√≥w na poziomie strony.
    - ZarzƒÖdzanie zapisem wsadowym (batching) wynik√≥w do bazy danych.
    - Aktualizowanie plik√≥w wej≈õciowych/wyj≈õciowych w trybie 'scan_all' lub 'scan_fix_file'.

    Args:
        process_mode (str): Tryb pracy ('full_scan', 'retry_errors', itp.).
        run_headless (bool): Czy uruchomiƒá przeglƒÖdarkƒô w trybie bez okna.
        input_file (str): ≈öcie≈ºka do pliku z URL-ami (u≈ºywana tylko w trybach
                          skanowania z pliku).
    """
    title_map = {
        'full_scan': "Doka≈Ñczanie Skanowania",
        'retry_errors': "Ponawianie B≈Çƒôd√≥w",
        'force_refresh': "Pe≈Çne Od≈õwie≈ºanie",
        'scan_all': f"Skanowanie z Pliku ({Path(input_file).name})",
        'scan_fix_file': f"Skanowanie z Pliku Naprawczego ({Path(input_file).name})"
    }
    logger.info(f"Uruchamiam Zaawansowany Skaner Online w trybie: [bold cyan]{title_map.get(process_mode)}[/bold cyan]", extra={"markup": True})

    # Krok 1: Pobierz listƒô URL-i do przetworzenia
    urls_to_process = await get_urls_for_processing(process_mode, input_file=input_file)
    if urls_to_process is None: # Krytyczny b≈ÇƒÖd odczytu
        console.print("[bold red]WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas przygotowywania listy URL-i. Sprawd≈∫ logi.[/bold red]")
        return
    if not urls_to_process:
        console.print("\n[bold green]‚úÖ Brak pracy do wykonania dla wybranych kryteri√≥w.[/bold green]")
        logger.info("Brak URL-i do przetworzenia. Ko≈Ñczƒô pracƒô.")
        return
        
    logger.info(f"Znaleziono [bold cyan]{len(urls_to_process)}[/bold cyan] URL-i do przetworzenia. Log: [yellow]{LOG_FILE.name}[/yellow]", extra={"markup": True})

    if process_mode in ['scan_all', 'scan_fix_file']:
        done_file_path = Path(input_file).with_suffix(f"{Path(input_file).suffix}_done")
        console.print(f"[dim]Przetworzone adresy bƒôdƒÖ przenoszone z '{Path(input_file).name}' do '{done_file_path.name}'.[/dim]")

    # Krok 2: Inicjalizacja zasob√≥w (przeglƒÖdarka, interfejs)
    p, browser, results_batch = None, None, []
    try:
        p = await async_playwright().start()
        browser = await getattr(p, BROWSER_TYPE).launch_persistent_context(Path(SESSION_DIR).expanduser(), headless=run_headless, args=BROWSER_ARGS.get(BROWSER_TYPE))
        page = await browser.new_page()
        
        if ENABLE_RESOURCE_BLOCKING:
            logger.info("Blokowanie zbƒôdnych zasob√≥w sieciowych jest [bold green]W≈ÅƒÑCZONE[/bold green].", extra={"markup": True})
            await page.route("**/*", block_unwanted_resources)

        # Inicjalizacja interfejsu Rich.Live
        counters = {"poprawne": 0, "b≈Çƒôdy": 0}
        recent_logs = deque(maxlen=5)
        progress_bar = Progress(TextColumn("[bold blue]{task.description}"), BarColumn(), "[progress.percentage]{task.percentage:>3.1f}%", "{task.completed}/{task.total}", TimeRemainingColumn())
        remaining_urls = list(urls_to_process)
        overall_task = progress_bar.add_task("[green]Postƒôp...", total=len(remaining_urls))
        layout = Layout()
        layout.split_column(Layout(progress_bar, name="progress", size=3), Layout(name="main_body"), Layout(name="footer", size=3))
        
        # Krok 3: G≈Ç√≥wna pƒôtla przetwarzania
        with Live(layout, screen=True, transient=True, auto_refresh=False) as live:
            for url in list(remaining_urls):
                if stop_event.is_set():
                    logger.warning("Przerwanie przez u≈ºytkownika. Zatrzymujƒô skanowanie...")
                    break

                photo_details, final_error = None, "Nieznany b≈ÇƒÖd"
                # Pƒôtla ponawiania pr√≥b dla pojedynczego URL
                for attempt in range(3):
                    if stop_event.is_set(): break
                    try:
                        await page.goto(url, wait_until='load', timeout=WAIT_FOR_PAGE_LOAD * 1000)
                        photo_details = await get_advanced_photo_details_from_page(page, url)
                        if photo_details: break # Sukces, przerywamy pƒôtlƒô ponawiania
                    except Exception as e:
                        final_error = str(e)
                        logger.warning(f"B≈ÇƒÖd podczas pr√≥by {attempt + 1} dla ...{url[-30:]}: {e}")
                
                if stop_event.is_set(): break
                
                # Przetwarzanie wyniku i aktualizacja UI
                if photo_details:
                    status = "Sukces"
                    counters["poprawne"] += 1
                    result_table = Table(show_header=False, box=None, padding=0, expand=True)
                    result_table.add_column(style="cyan", justify="right", width=25)
                    result_table.add_column()
                    for key, value in photo_details.items():
                        if isinstance(value, list): result_table.add_row(f"{key}:", "\n".join(f"- {item}" for item in value))
                        elif isinstance(value, dict): result_table.add_row(f"{key}:", json.dumps(value, indent=2, ensure_ascii=False))
                        else: result_table.add_row(f"{key}:", str(value))
                    recent_logs.appendleft(Panel(result_table, title=f"[bold green]‚úÖ Sukces![/] ...{url[-30:]}"))
                else:
                    status = "B≈ÇƒÖd"
                    photo_details = {"error": final_error}
                    counters["b≈Çƒôdy"] += 1
                    recent_logs.appendleft(Panel(f"Nie uda≈Ço siƒô pobraƒá danych. Ostatni b≈ÇƒÖd: {final_error[:100]}...", title=f"[bold red]‚ùå B≈ÇƒÖd![/] ...{url[-30:]}", border_style="red"))
                
                # Dodaj wynik do partii do zapisu w bazie
                results_batch.append({
                    "url": url,
                    "metadata_json": json.dumps(photo_details, ensure_ascii=False) if photo_details else None,
                    "processing_status": status,
                    "expected_path": photo_details.get('expected_path') if photo_details else None
                })
                log_to_file(url, photo_details, status)
                
                # Zapisz partiƒô do bazy, je≈õli osiƒÖgnƒô≈Ça odpowiedni rozmiar
                if len(results_batch) >= BATCH_SIZE:
                    await update_scanned_entries_batch(results_batch)
                    results_batch.clear()

                # Aktualizacja plik√≥w postƒôpu w trybie skanowania z pliku
                if process_mode in ['scan_all', 'scan_fix_file']:
                    try:
                        remaining_urls.remove(url)
                        with open(done_file_path, "a", encoding="utf-8") as f_done: f_done.write(f"{url}\n")
                        with open(input_file, "w", encoding="utf-8") as f_input:
                            for rem_url in remaining_urls: f_input.write(f"{rem_url}\n")
                    except (IOError, ValueError) as e:
                        logger.error(f"B≈ÇƒÖd podczas aktualizacji plik√≥w postƒôpu: {e}")
                
                # Aktualizacja interfejsu
                progress_bar.update(overall_task, advance=1)
                layout["main_body"].update(Panel(Group(*recent_logs), title="Ostatnie Akcje"))
                counters_table = Table.grid(expand=True); counters_table.add_column(justify="center"); counters_table.add_column(justify="center")
                counters_table.add_row(f"[green]Poprawne: {counters['poprawne']}[/]", f"[red]B≈Çƒôdy: {counters['b≈Çƒôdy']}[/]")
                layout["footer"].update(Panel(counters_table, title="Statystyki Sesji"))
                live.refresh()

        # Zapisz ostatniƒÖ partiƒô danych, je≈õli jaka≈õ zosta≈Ça
        if results_batch:
            await update_scanned_entries_batch(results_batch)
            
    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç nieobs≈Çugiwany b≈ÇƒÖd w g≈Ç√≥wnej pƒôtli skanera: {e}", exc_info=True)
    finally:
        logger.info("Zamykanie zasob√≥w skanera online...")
        if browser: await browser.close()
        if p: await p.stop()
        logger.info("Zasoby skanera zwolnione.")

# plik: core/advanced_scanner_logic.py

async def run_offline_file_corrector():
    """
    Skaner Offline z SamokorektƒÖ Lokalizacji Plik√≥w.

    Narzƒôdzie to wykonuje nastƒôpujƒÖce kroki:
    1.  Pobiera z bazy danych wszystkie wpisy, kt√≥re majƒÖ zdefiniowane
        zar√≥wno `final_path` (rzeczywista lokalizacja), jak i `expected_path`
        (obliczona przez skaner online).
    2.  Iteruje przez ka≈ºdy wpis, por√≥wnujƒÖc obie ≈õcie≈ºki.
    3.  Je≈õli ≈õcie≈ºki siƒô nie zgadzajƒÖ, a plik ≈∫r√≥d≈Çowy (`final_path`) istnieje:
        a) Przenosi plik z jego aktualnej lokalizacji do lokalizacji oczekiwanej.
        b) Aktualizuje wpis w bazie danych, aby `final_path` by≈Ç zgodny z nowƒÖ,
           poprawnƒÖ lokalizacjƒÖ.
    4.  Na ko≈Ñcu wy≈õwietla podsumowanie wykonanych operacji.
    """
    console.clear()
    logger.info("Uruchamiam Skaner Offline z SamokorektƒÖ Lokalizacji Plik√≥w...")
    console.print(Panel("üõ∞Ô∏è  Korektor Lokalizacji Plik√≥w (Offline) üõ∞Ô∏è", expand=False, style="green"))
    
    try:
        # Krok 1: Pobierz dane do weryfikacji
        # W przysz≈Ço≈õci ta logika zostanie przeniesiona do dedykowanej funkcji w database.py
        async with aiosqlite.connect(DATABASE_FILE) as conn:
            conn.row_factory = aiosqlite.Row
            query = "SELECT id, final_path, expected_path FROM downloaded_media WHERE final_path IS NOT NULL AND final_path != '' AND expected_path IS NOT NULL AND expected_path != ''"
            cursor = await conn.execute(query)
            records_to_check = await cursor.fetchall()
        
        if not records_to_check:
            logger.warning("Nie znaleziono plik√≥w do weryfikacji. Uruchom najpierw skaner online, aby wygenerowaƒá oczekiwane ≈õcie≈ºki.")
            console.print("\n[bold yellow]Nie znaleziono plik√≥w do weryfikacji. Uruchom skaner online.[/bold yellow]")
            return

        logger.info(f"Znaleziono {len(records_to_check)} plik√≥w do weryfikacji lokalizacji.")
        moved_count, error_count, skipped_count = 0, 0, 0
        
        # Krok 2: Iteruj i naprawiaj niesp√≥jno≈õci
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[green]Weryfikacja lokalizacji plik√≥w...", total=len(records_to_check))
            for record in records_to_check:
                try:
                    final_path = Path(record['final_path'])
                    expected_path = Path(record['expected_path'])

                    # Sprawd≈∫, czy ≈õcie≈ºka nie jest po prostu katalogiem (np. '.')
                    if not final_path.name or not expected_path.name:
                        logger.warning(f"Pominiƒôto rekord ID {record['id']} z powodu nieprawid≈Çowej ≈õcie≈ºki.")
                        skipped_count += 1
                        continue

                    if final_path.resolve() != expected_path.resolve():
                        console.print(f"\n[yellow]Niesp√≥jno≈õƒá wykryta dla ID {record['id']}:[/]")
                        console.print(f"  [dim]Jest w:[/dim] {final_path}")
                        console.print(f"  [cyan]Powinien byƒá w:[/cyan] {expected_path}")

                        # U≈ºyj asyncio.to_thread do wykonania blokujƒÖcych operacji na plikach
                        if not await asyncio.to_thread(final_path.exists):
                            logger.error(f"B≈ÅƒÑD: Plik ≈∫r√≥d≈Çowy {final_path} nie istnieje. Pomijam.")
                            error_count += 1
                            continue

                        await asyncio.to_thread(expected_path.parent.mkdir, parents=True, exist_ok=True)
                        await asyncio.to_thread(shutil.move, final_path, expected_path)
                        
                        # Zaktualizuj wpis w bazie danych
                        async with aiosqlite.connect(DATABASE_FILE) as conn_update:
                            await conn_update.execute("UPDATE downloaded_media SET final_path = ? WHERE id = ?", (str(expected_path), record['id']))
                            await conn_update.commit()
                        
                        console.print(f"  [bold green]Sukces: Plik zosta≈Ç przeniesiony.[/bold green]")
                        moved_count += 1
                except Exception as e:
                    logger.error(f"B≈ÅƒÑD podczas przenoszenia pliku dla ID {record['id']}: {e}", exc_info=True)
                    error_count += 1
                finally:
                    progress.update(task, advance=1)

        # Krok 3: Wy≈õwietl podsumowanie
        logger.info("Zako≈Ñczono weryfikacjƒô lokalizacji plik√≥w.")
        console.print("\n[bold green]Zako≈Ñczono weryfikacjƒô lokalizacji plik√≥w.[/bold green]")
        console.print(f"  - Przeniesiono plik√≥w: [cyan]{moved_count}[/cyan]")
        console.print(f"  - Pominiƒôto (b≈Çƒôdne dane): [yellow]{skipped_count}[/yellow]")
        console.print(f"  - B≈Çƒôdy: [red]{error_count}[/red]")

    except aiosqlite.Error as e:
        logger.critical(f"B≈ÇƒÖd bazy danych podczas korekty plik√≥w: {e}", exc_info=True)


async def run_filename_fixer_from_db():
    """
    Skaner Offline: Naprawia nazwy plik√≥w na dysku na podstawie metadanych,
    inteligentnie rozwiƒÖzujƒÖc konflikty i w pe≈Çni synchronizujƒÖc wpisy w bazie.

    Proces:
    1.  Uruchamia pƒôtlƒô, kt√≥ra dzia≈Ça do momentu, a≈º nie zostanƒÖ znalezione
        ≈ºadne niesp√≥jno≈õci.
    2.  W ka≈ºdej iteracji, pobiera z bazy pliki, kt√≥rych nazwa na dysku
        (`filename`) r√≥≈ºni siƒô od nazwy w metadanych (`metadata_json.FileName`).
    3.  Dla ka≈ºdej niesp√≥jno≈õci:
        a) Generuje nowƒÖ, poprawnƒÖ ≈õcie≈ºkƒô, u≈ºywajƒÖc `create_unique_filepath`
           do automatycznego rozwiƒÖzania ewentualnych konflikt√≥w nazw (np.
           dodajƒÖc `_1`, `_2`).
        b) Zmienia nazwƒô pliku na dysku.
        c) Aktualizuje WSZYSTKIE powiƒÖzane pola w bazie (`filename`, `final_path`,
           `expected_path`, `metadata_json`), aby zapewniƒá pe≈ÇnƒÖ sp√≥jno≈õƒá.
    """
    console.clear()
    logger.info("Uruchamiam narzƒôdzie do naprawy nazw plik√≥w na podstawie metadanych...")
    console.print(Panel("[bold yellow]Naprawa Nazw Plik√≥w z Pe≈ÇnƒÖ SynchronizacjƒÖ[/]", expand=False))
    
    if not Confirm.ask("\n[bold red]UWAGA:[/bold red] Ta operacja zmieni nazwy plik√≥w na Twoim dysku. Czy na pewno chcesz kontynuowaƒá?", default=False):
        logger.warning("Naprawa nazw plik√≥w anulowana przez u≈ºytkownika.")
        return

    try:
        run_count = 0
        while True:
            run_count += 1
            logger.info(f"Rozpoczynam przebieg {run_count} weryfikacji nazw plik√≥w.")
            
            async with aiosqlite.connect(DATABASE_FILE) as conn:
                conn.row_factory = aiosqlite.Row
                query = """
                    SELECT id, filename, final_path, expected_path, metadata_json FROM downloaded_media
                    WHERE status = 'downloaded' AND json_valid(metadata_json) = 1
                    AND json_extract(metadata_json, '$.FileName') IS NOT NULL
                """
                cursor = await conn.execute(query)
                records_to_check = await cursor.fetchall()
            
            if not records_to_check:
                logger.info("Nie znaleziono plik√≥w z metadanymi do weryfikacji nazw.")
                break

            mismatches_found_this_run = False
            with Progress(console=console, transient=True) as progress:
                task = progress.add_task(f"[green]Weryfikacja nazw (przebieg {run_count})...[/]", total=len(records_to_check))
                for record in records_to_check:
                    progress.update(task, advance=1)
                    try:
                        current_path = Path(record['final_path'])
                        metadata = json.loads(record['metadata_json'])
                        filename_from_meta = metadata.get('FileName')
                        
                        if not await asyncio.to_thread(current_path.exists) or not filename_from_meta or current_path.name == filename_from_meta:
                            continue

                        mismatches_found_this_run = True
                        
                        new_path = create_unique_filepath(current_path.parent, filename_from_meta)
                        new_filename = new_path.name

                        console.print(f"\n[yellow]Niesp√≥jno≈õƒá nazwy dla ID {record['id']}:[/]")
                        console.print(f"  [dim]Aktualna nazwa:[/dim] {current_path.name}")
                        console.print(f"  [cyan]Oczekiwana nazwa:[/cyan] {filename_from_meta}")
                        if new_filename != filename_from_meta:
                            console.print(f"  [magenta]Kolizja! Zmieniam nazwƒô na:[/magenta] {new_filename}")

                        await asyncio.to_thread(current_path.rename, new_path)
                        
                        if new_filename != filename_from_meta:
                            metadata['FileName'] = new_filename
                        
                        if 'DateTime' in metadata:
                            dt = datetime.fromisoformat(metadata["DateTime"])
                            dest_dir = Path(DOWNLOADS_DIR_BASE) / str(dt.year) / f"{dt.month:02d}"
                            new_expected_path = str(dest_dir / new_filename)
                            metadata['expected_path'] = new_expected_path
                        else:
                            new_expected_path = str(new_path)

                        async with aiosqlite.connect(DATABASE_FILE) as conn_update:
                            await conn_update.execute(
                                "UPDATE downloaded_media SET filename = ?, final_path = ?, expected_path = ?, metadata_json = ? WHERE id = ?",
                                (new_filename, str(new_path), new_expected_path, json.dumps(metadata, ensure_ascii=False), record['id'])
                            )
                            await conn_update.commit()
                        
                        logger.info(f"Zsynchronizowano plik ID {record['id']}. Nowa nazwa: '{new_filename}'.")
                        console.print("  [bold green]Sukces: Plik i wpis w bazie zosta≈Çy w pe≈Çni zsynchronizowane.[/bold green]")
                    
                    except Exception as e:
                        logger.error(f"B≈ÇƒÖd podczas naprawy nazwy dla pliku {record['final_path']}", exc_info=True)
            
            if not mismatches_found_this_run:
                logger.info("Brak dalszych niesp√≥jno≈õci. Ko≈Ñczƒô pƒôtlƒô naprawczƒÖ.")
                break
                
        logger.info("Zako≈Ñczono naprawƒô nazw plik√≥w.")
        console.print("\n[bold green]Zako≈Ñczono. Wszystkie nazwy plik√≥w sƒÖ teraz sp√≥jne z metadanymi.[/bold green]")
        
    except aiosqlite.Error as e:
        logger.critical("B≈ÇƒÖd bazy danych podczas naprawy nazw plik√≥w.", exc_info=True)


async def run_metadata_completer():
    """
    Skaner Offline: Uzupe≈Çnia brakujƒÖce dane i oblicza `expected_path`.

    Narzƒôdzie to znajduje w bazie pliki, kt√≥re zosta≈Çy pobrane, ale z powodu
    niekompletnych danych ze skanera online (lub ich braku) nie majƒÖ
    obliczonej oczekiwanej ≈õcie≈ºki (`expected_path`).

    Dla ka≈ºdego takiego pliku:
    1.  Wczytuje jego metadane z Exiftool.
    2.  ≈ÅƒÖczy je z istniejƒÖcymi danymi w bazie (dane z Exif uzupe≈ÇniajƒÖ braki).
    3.  Oblicza poprawnƒÖ `expected_path` na podstawie daty i nazwy pliku.
    4.  Aktualizuje rekord w bazie danych o kompletne metadane.
    """
    console.clear()
    logger.info("Uruchamiam narzƒôdzie do uzupe≈Çniania metadanych i oczekiwanych ≈õcie≈ºek...")
    console.print(Panel("[bold blue]Uzupe≈Çniacz Danych i Oczekiwanych ≈öcie≈ºek[/]", expand=False))
    
    if not EXIFTOOL_AVAILABLE:
        logger.critical("Brak biblioteki 'pyexiftool'. Operacja zosta≈Ça przerwana.")
        console.print(Panel("[bold red]B≈ÇƒÖd: Brak wymaganej biblioteki 'pyexiftool'![/bold red]\n\nUruchom: [cyan]pip install pyexiftool[/cyan]", title="Instrukcja Instalacji"))
        return

    try:
        async with aiosqlite.connect(DATABASE_FILE) as conn:
            conn.row_factory = aiosqlite.Row
            query = """
                SELECT id, final_path, metadata_json FROM downloaded_media
                WHERE status = 'downloaded' AND json_valid(metadata_json) = 1
                AND (expected_path IS NULL OR expected_path = '')
            """
            cursor = await conn.execute(query)
            records_to_fix = await cursor.fetchall()
        
        if not records_to_fix:
            logger.info("Nie znaleziono plik√≥w wymagajƒÖcych uzupe≈Çnienia danych.")
            console.print("\n[bold green]‚úÖ Wszystkie pobrane pliki majƒÖ ju≈º obliczonƒÖ oczekiwanƒÖ ≈õcie≈ºkƒô.[/bold green]")
            return

        logger.info(f"Znaleziono {len(records_to_fix)} plik√≥w do uzupe≈Çnienia danych.")
        if not Confirm.ask("\n[cyan]Czy chcesz kontynuowaƒá?[/]", default=True):
            logger.warning("Operacja uzupe≈Çniania danych anulowana."); return

        fixed_count, error_count = 0, 0
        loop = asyncio.get_running_loop()
        
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[green]Uzupe≈Çnianie danych...", total=len(records_to_fix))
            for record in records_to_fix:
                try:
                    current_path = Path(record['final_path'])
                    if not await asyncio.to_thread(current_path.exists):
                        logger.warning(f"Plik {current_path} nie istnieje na dysku. Pomijam.")
                        continue

                    existing_metadata = json.loads(record['metadata_json'])
                    
                    # Uruchom blokujƒÖcƒÖ operacjƒô ExifTool w osobnym wƒÖtku
                    with exiftool.ExifToolHelper() as et:
                        exif_metadata = await loop.run_in_executor(None, et.get_metadata, str(current_path))
                    
                    if not exif_metadata:
                         logger.warning(f"Nie uda≈Ço siƒô odczytaƒá metadanych Exif dla {current_path.name}."); continue

                    # Po≈ÇƒÖcz metadane (dane z Exif uzupe≈ÇniajƒÖ braki)
                    merged_metadata = exif_metadata[0].copy()
                    merged_metadata.update(existing_metadata)

                    if 'DateTime' not in merged_metadata or not merged_metadata['DateTime']:
                        date_obj = await get_date_from_metadata(merged_metadata)
                        if date_obj: merged_metadata['DateTime'] = date_obj.isoformat()
                    
                    if 'FileName' not in merged_metadata or not merged_metadata['FileName']:
                         merged_metadata['FileName'] = merged_metadata.get('File:FileName', current_path.name)
                         
                    if 'DateTime' in merged_metadata and 'FileName' in merged_metadata:
                        dt = datetime.fromisoformat(merged_metadata["DateTime"])
                        dest_dir = Path(DOWNLOADS_DIR_BASE) / str(dt.year) / f"{dt.month:02d}"
                        expected_path = str(dest_dir / merged_metadata['FileName'])
                        
                        async with aiosqlite.connect(DATABASE_FILE) as conn_update:
                            await conn_update.execute(
                                "UPDATE downloaded_media SET metadata_json = ?, expected_path = ? WHERE id = ?",
                                (json.dumps(merged_metadata, ensure_ascii=False), expected_path, record['id'])
                            )
                            await conn_update.commit()
                        fixed_count += 1
                    else:
                        logger.error(f"Nie uda≈Ço siƒô ustaliƒá daty lub nazwy pliku dla ID {record['id']}.")
                        error_count += 1

                except Exception as e:
                    logger.error(f"B≈ÇƒÖd podczas przetwarzania pliku {record['final_path']}", exc_info=True)
                    error_count += 1
                finally:
                    progress.update(task, advance=1)
                        
        logger.info("Zako≈Ñczono uzupe≈Çnianie danych.")
        console.print(f"\n[bold green]Zako≈Ñczono. Uzupe≈Çniono dane dla [cyan]{fixed_count}[/cyan] plik√≥w. B≈Çƒôdy: [red]{error_count}[/red].[/bold green]")

    except aiosqlite.Error as e:
        logger.critical("B≈ÇƒÖd bazy danych podczas uzupe≈Çniania danych.", exc_info=True)


# ##############################################################################
# ===                  SEKCJA 3: NARZƒòDZIA ZARZƒÑDCZE I DIAGNOSTYCZNE         ===
# ##############################################################################

async def write_metadata_from_db_to_files():
    """
    Odczytuje metadane z bazy danych i zapisuje je do plik√≥w na dysku za
    pomocƒÖ Exiftool.

    UWAGA: Ta operacja jest nieodwracalna i trwale modyfikuje pliki
    multimedialne na dysku twardym. Zaleca siƒô wykonanie kopii zapasowej
    przed jej uruchomieniem.

    Proces:
    1.  Pobiera z bazy wszystkie wpisy, kt√≥re majƒÖ status 'downloaded' i
        posiadajƒÖ metadane w formacie JSON.
    2.  Dla ka≈ºdego pliku, t≈Çumaczy dane z JSON (np. 'Description',
        'TaggedPeople', 'GPS') na standardowe tagi EXIF/IPTC/XMP.
    3.  Wywo≈Çuje zewnƒôtrzny program `exiftool` w celu "wypalenia" tych
        tag√≥w bezpo≈õrednio w pliku.
    """
    console.clear()
    logger.info("Uruchamiam narzƒôdzie do zapisu metadanych w plikach (Exiftool)...")
    console.print(Panel("‚úçÔ∏è Zapisywanie Metadanych z Bazy do Plik√≥w (Exiftool)", expand=False, style="red"))
    
    if not EXIFTOOL_AVAILABLE:
        logger.critical("Brak biblioteki 'pyexiftool'. Operacja zosta≈Ça przerwana.")
        console.print(Panel("[bold red]B≈ÇƒÖd: Brak wymaganej biblioteki 'pyexiftool'![/bold red]\n\nUruchom: [cyan]pip install pyexiftool[/cyan]", title="Instrukcja Instalacji"))
        return
        
    console.print("\n[bold yellow]‚ö†Ô∏è UWAGA: Ta operacja nieodwracalnie zmodyfikuje pliki na dysku![/bold yellow]")
    if not Confirm.ask("Czy na pewno chcesz kontynuowaƒá? (Zalecana jest kopia zapasowa)", default=False):
        logger.warning("Operacja zapisu metadanych anulowana przez u≈ºytkownika.")
        return

    try:
        async with aiosqlite.connect(DATABASE_FILE) as conn:
            query = "SELECT final_path, metadata_json FROM downloaded_media WHERE status = 'downloaded' AND metadata_json IS NOT NULL AND metadata_json != '{}'"
            cursor = await conn.execute(query)
            records_to_process = await cursor.fetchall()
    except aiosqlite.Error as e:
        logger.critical(f"B≈ÇƒÖd odczytu z bazy danych: {e}", exc_info=True)
        return

    if not records_to_process:
        logger.warning("Nie znaleziono przetworzonych plik√≥w z metadanymi w bazie danych do zapisu.")
        console.print("\n[bold yellow]Nie znaleziono plik√≥w z metadanymi do zapisu.[/bold yellow]")
        return

    success_count, error_count, skipped_count = 0, 0, 0
    loop = asyncio.get_running_loop()

    with Progress(console=console, transient=True) as progress:
        task = progress.add_task("[green]Zapisywanie tag√≥w w plikach...", total=len(records_to_process))
        for file_path_str, metadata_json in records_to_process:
            try:
                file_path = Path(file_path_str)
                if not await asyncio.to_thread(file_path.exists):
                    logger.warning(f"Pominiƒôto: Plik nie istnieje {file_path}")
                    skipped_count += 1
                    continue

                data = json.loads(metadata_json)
                
                tags_to_write = {}
                if data.get("Description"):
                    tags_to_write["EXIF:ImageDescription"] = data["Description"]
                    tags_to_write["IPTC:Caption-Abstract"] = data["Description"]
                if data.get("TaggedPeople"):
                    tags_to_write["IPTC:Keywords"] = data["TaggedPeople"]
                if data.get("DateTime"):
                    dt_str = data["DateTime"].replace("T", " ")
                    tags_to_write["EXIF:DateTimeOriginal"] = dt_str
                    tags_to_write["EXIF:CreateDate"] = dt_str
                if data.get("GPS"):
                    tags_to_write.update({
                        "EXIF:GPSLatitude": data["GPS"]["latitude"],
                        "EXIF:GPSLongitude": data["GPS"]["longitude"],
                        "EXIF:GPSLatitudeRef": "N" if data["GPS"]["latitude"] >= 0 else "S",
                        "EXIF:GPSLongitudeRef": "E" if data["GPS"]["longitude"] >= 0 else "W"
                    })
                
                if not tags_to_write:
                    skipped_count += 1; continue

                params = []
                for tag, value in tags_to_write.items():
                    if isinstance(value, list):
                        for v in value: params.extend([f"-{tag}={v}"])
                    else:
                        params.extend([f"-{tag}={value}"])
                
                # Uruchom blokujƒÖcƒÖ operacjƒô Exiftool w osobnym wƒÖtku
                with exiftool.ExifToolHelper() as et:
                    await loop.run_in_executor(None, et.execute, "-overwrite_original", "-m", *params, str(file_path))
                
                logger.debug(f"Pomy≈õlnie zapisano {len(tags_to_write)} tag√≥w do pliku {file_path.name}")
                success_count += 1

            except json.JSONDecodeError:
                logger.error(f"B≈ÇƒÖd: Uszkodzony JSON dla pliku {file_path_str}")
                error_count += 1
            except Exception as e:
                logger.error(f"B≈ÇƒÖd zapisu do pliku {file_path_str}: {e}", exc_info=True)
                error_count += 1
            finally:
                progress.update(task, advance=1)

    logger.info("Zako≈Ñczono zapisywanie metadanych do plik√≥w.")
    console.print("\n[bold green]Zako≈Ñczono zapisywanie metadanych do plik√≥w.[/bold green]")
    console.print(f"  - Zapisano pomy≈õlnie: [cyan]{success_count}[/cyan]")
    console.print(f"  - Pominiƒôto (brak danych/pliku): [yellow]{skipped_count}[/yellow]")
    console.print(f"  - B≈Çƒôdy: [red]{error_count}[/red]")


async def test_single_url_diagnostics(run_headless: bool):
    """
    Uruchamia pe≈Çny test diagnostyczny dla jednego, rƒôcznie podanego adresu URL.

    Funkcja ta wykonuje kluczowƒÖ operacjƒô diagnostycznƒÖ:
    1.  Uruchamia najnowszƒÖ wersjƒô skanera `get_advanced_photo_details_from_page`
        dla podanego URL-a.
    2.  Wy≈õwietla wszystkie zebrane metadane w czytelnej tabeli.

    Jest to niezbƒôdne narzƒôdzie do diagnozowania problem√≥w ze skanerem.
    Je≈õli po zmianach na stronie Google kt√≥re≈õ pole metadanych przesta≈Ço
    byƒá pobierane, ten test natychmiast to poka≈ºe, pozwalajƒÖc deweloperowi
    skupiƒá siƒô na naprawie odpowiedniego selektora wewnƒÖtrz funkcji
    `get_advanced_photo_details_from_page`.

    Args:
        run_headless (bool): Czy uruchomiƒá przeglƒÖdarkƒô w trybie bez okna.
    """
    console.clear()
    console.print(Panel("[bold yellow]üî¨ Test Skanera Online dla Pojedynczego URL üî¨[/]", expand=False))
    url = Prompt.ask("\n[cyan]Wklej adres URL zdjƒôcia, kt√≥ry chcesz przetestowaƒá[/]")
    if not url.strip().startswith("http"):
        logger.error("To nie jest prawid≈Çowy adres URL.")
        return

    logger.info(f"Uruchamianie przeglƒÖdarki w trybie testowym (headless: {run_headless})...")
    
    async with async_playwright() as p:
        browser = None
        try:
            browser = await getattr(p, BROWSER_TYPE).launch_persistent_context(
                Path(SESSION_DIR).expanduser(), headless=run_headless, args=BROWSER_ARGS.get(BROWSER_TYPE)
            )
            page = await browser.new_page()

            if ENABLE_RESOURCE_BLOCKING:
                logger.info("Blokowanie zasob√≥w w≈ÇƒÖczone na czas testu.")
                await page.route("**/*", block_unwanted_resources)

            with console.status(f"[cyan]Nawigacja do: [dim]{url}[/dim]...[/]"):
                await page.goto(url, wait_until='load', timeout=WAIT_FOR_PAGE_LOAD * 1000)
            
            logger.info("Strona za≈Çadowana. Uruchamiam skaner g≈Ç√≥wny...")
            with console.status("[cyan]Skanowanie metadanych ze strony...[/]"):
                metadata = await get_advanced_photo_details_from_page(page, url)
            
            console.clear()
            if metadata:
                console.print(Panel("[bold green]‚úÖ SKANER ZAKO≈ÉCZY≈Å PRACƒò SUKCESEM[/]", title="Wynik Testu"))
                table = Table(title="Zebrane Metadane", show_header=False, box=None, padding=(0, 2))
                table.add_column(style="cyan", justify="right", width=25)
                table.add_column()
                for key, value in metadata.items():
                    if isinstance(value, list):
                        table.add_row(f"[bold]{key}:[/bold]", "\n".join(f"- {item}" for item in value))
                    elif isinstance(value, dict):
                        table.add_row(f"[bold]{key}:[/bold]", json.dumps(value, indent=2, ensure_ascii=False))
                    else:
                        table.add_row(f"[bold]{key}:[/bold]", str(value))
                console.print(Panel(table, border_style="green"))
            else:
                console.print(Panel("[bold red]‚ùå SKANER ZAKO≈ÉCZY≈Å PRACƒò B≈ÅƒòDEM LUB NIE ZNALAZ≈Å DANYCH[/]", title="Wynik Testu", border_style="red"))
                logger.error("Skaner nie zwr√≥ci≈Ç ≈ºadnych metadanych.")

        except Exception as e:
            logger.critical(f"WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas testu: {e}", exc_info=True)
            console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd krytyczny. Sprawd≈∫ logi.[/bold red]")
        finally:
            if browser:
                await browser.close()
            logger.info("Test zako≈Ñczony, przeglƒÖdarka zamkniƒôta.")


# ##############################################################################
# ===                    SEKCJA 4: G≈Å√ìWNA FUNKCJA URUCHOMIENIOWA             ===
# ##############################################################################

async def run_advanced_scanner():
    """
    Wy≈õwietla i zarzƒÖdza interaktywnym menu dla wszystkich funkcji
    dostƒôpnych w module Zaawansowanego Skanera i Mened≈ºera Kolekcji.
    """
    logger.info("Uruchamiam menu Zaawansowanego Skanera i Mened≈ºera Kolekcji.")
    
    # Upewnij siƒô, ≈ºe baza danych i jej struktura sƒÖ gotowe do pracy
    await setup_database()

    while True:
        console.clear()
        
        # Definicja opcji w menu z podzia≈Çem na logiczne sekcje
        menu_items = [
            ("--- G≈Å√ìWNY PRZEP≈ÅYW PRACY (Zalecana kolejno≈õƒá) ---", None),
            ("Krok 1: Doko≈Ñcz skanowanie metadanych z bazy (Online)", "full_scan"),
            ("Krok 2: Uzupe≈Çnij dane i ≈õcie≈ºki z plik√≥w (Offline)", "complete_metadata"),
            ("Krok 3: Sprawd≈∫ i napraw LOKALIZACJE plik√≥w (Offline)", "correct_paths"),
            ("Krok 4: Sprawd≈∫ i napraw NAZWY plik√≥w (Offline)", "fix_filenames"),

            ("--- ZAAWANSOWANE OPERACJE ONLINE ---", None),
            ("Pon√≥w tylko te URL-e z bazy, kt√≥re mia≈Çy b≈ÇƒÖd", "retry_errors"),
            ("Od≈õwie≈º metadane dla WSZYSTKICH wpis√≥w w bazie", "force_refresh"),
            ("Skanuj wszystkie URL-e z pliku 'urls_to_scan.txt'", "scan_all"),
            ("Skanuj URL-e wymagajƒÖce naprawy z pliku 'urls_to_fix.txt'", "scan_fix_file"),

            ("--- NARZƒòDZIA POMOCNICZE I DIAGNOSTYKA ---", None),
            ("ZAPISZ metadane z bazy do plik√≥w (Exiftool)", "write_to_files"),
            ("Wygeneruj plik 'urls_to_scan.txt' z bazy", "export_urls"),
            ("Wygeneruj plik 'urls_to_fix.txt' (z brakujƒÖcymi metadanymi)", "export_fix_urls"),
            ("Uruchom PE≈ÅNY TEST (Skaner + Diagnostyka)", "single_url_test"),
            
            ("---", None),
            ("Wr√≥ƒá do menu g≈Ç√≥wnego", "exit")
        ]

        selected_mode = await create_interactive_menu(
            menu_items,
            "Zaawansowany Skaner i Mened≈ºer Kolekcji",
            border_style="magenta"
        )
        
        if selected_mode == "exit" or selected_mode is None:
            logger.info("Anulowano. Powr√≥t do menu g≈Ç√≥wnego.")
            break
        
        logger.info(f"U≈ºytkownik wybra≈Ç opcjƒô: '{selected_mode}'")
        
        online_modes = ['full_scan', 'retry_errors', 'force_refresh', 'scan_all', 'scan_fix_file']
        
        # Wywo≈Çaj odpowiedniƒÖ funkcjƒô na podstawie wyboru u≈ºytkownika
        if selected_mode in online_modes:
            input_file = "urls_to_fix.txt" if selected_mode == 'scan_fix_file' else URL_INPUT_FILE
            run_headless = Confirm.ask("Uruchomiƒá w trybie niewidocznym (headless)?", default=DEFAULT_HEADLESS_MODE)
            await run_scanner_core(process_mode=selected_mode, run_headless=run_headless, input_file=input_file)
        elif selected_mode == 'correct_paths':
            await run_offline_file_corrector()
        elif selected_mode == 'fix_filenames':
            await run_filename_fixer_from_db()
        elif selected_mode == 'complete_metadata':
            await run_metadata_completer()
        elif selected_mode == 'write_to_files':
            if not EXIFTOOL_AVAILABLE:
                console.print(Panel("[bold red]B≈ÇƒÖd: Brak wymaganej biblioteki 'pyexiftool'![/bold red]\n\nUruchom: [cyan]pip install pyexiftool[/cyan]", title="Instrukcja Instalacji"))
            else:
                await write_metadata_from_db_to_files()
        elif selected_mode == 'export_urls':
            await export_urls_from_db_to_file()
        elif selected_mode == 'export_fix_urls':
            await export_fix_needed_urls_to_file()
        elif selected_mode == 'single_url_test':
            await test_single_url_diagnostics(run_headless=False)
        
        Prompt.ask("\n[bold]Operacja zako≈Ñczona. Naci≈õnij Enter, aby wr√≥ciƒá do menu skanera...[/]", console=console)
