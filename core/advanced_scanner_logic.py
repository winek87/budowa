# plik: core/advanced_scanner_logic.py
# Wersja 7.0 - Scentralizowana logika bazy danych i ujednolicone ≈õcie≈ºki (Refaktoryzacja Fazy 1)

# -*- coding: utf-8 -*-

# plik: core/advanced_scanner_logic.py
# Wersja 6.0 - W pe≈Çni asynchroniczny, udokumentowany i zintegrowany z nowym modu≈Çem bazy danych
#
# ##############################################################################
# ===                        JAK TO DZIA≈ÅA (PROSTE WYJA≈öNIENIE)                ===
# ##############################################################################
#
# Ten plik zawiera logikƒô zaawansowanego, wielofunkcyjnego narzƒôdzia do
# zarzƒÖdzania metadanymi i sp√≥jno≈õciƒÖ kolekcji. Pe≈Çni trzy g≈Ç√≥wne role:
#
#  1. SKANER ONLINE: Pobiera bogate metadane (opisy, albumy, tagi, GPS)
#     bezpo≈õrednio ze strony Google Photos i oblicza OCZEKIWANƒÑ, idealnƒÖ
#     lokalizacjƒô pliku (`expected_path`) na dysku.
#
#  2. KOREKTOR OFFLINE: Por√≥wnuje rzeczywistƒÖ lokalizacjƒô pobranych plik√≥w
#     z ich oczekiwanƒÖ lokalizacjƒÖ i pozwala na ich automatycznƒÖ naprawƒô.
#
#  3. ZAPISYWARKA EXIF: Odczytuje metadane z bazy danych i zapisuje je
#     bezpo≈õrednio w plikach na dysku.
#
################################################################################

# --- G≈Å√ìWNE IMPORTY ---
import asyncio
import json
import re
import shutil
import logging
from collections import deque
from datetime import datetime
from pathlib import Path
import aiosqlite

# --- Zale≈ºno≈õci zewnƒôtrzne (opcjonalne) ---
try:
    import exiftool
    EXIFTOOL_AVAILABLE = True
except ImportError:
    EXIFTOOL_AVAILABLE = False
    
# --- Playwright ---
from playwright.async_api import async_playwright, Page

# --- Importy z `rich` ---
from rich.console import Console, Group
from rich.panel import Panel
from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn
from rich.live import Live
from rich.layout import Layout
from rich.table import Table
from rich.text import Text
from rich.prompt import Confirm, Prompt

# --- IMPORTY Z W≈ÅASNYCH MODU≈Å√ìW `core` ---
# Jawne importy z pliku konfiguracyjnego
from .config import (
    DATABASE_FILE, SESSION_DIR, DOWNLOADS_DIR_BASE, URL_INPUT_FILE,
    WAIT_FOR_SELECTOR, WAIT_FOR_PAGE_LOAD, BROWSER_TYPE, BROWSER_ARGS,
    ENABLE_RESOURCE_BLOCKING, INFO_PANEL_BUTTON_SELECTOR,
    DEFAULT_HEADLESS_MODE, BLOCKED_RESOURCE_TYPES
)

# NOWE, SCENTRALIZOWANE IMPORTY Z MODU≈ÅU BAZY DANYCH
from .database import (
    setup_database,
    get_urls_for_online_scan,
    update_scanned_entries_batch,
    get_urls_to_fix,
    get_all_urls_from_db,
    get_records_for_path_correction,
    update_final_path,
    get_records_for_filename_fix,
    update_entry_after_rename,
    get_records_for_metadata_completion,
    update_entry_with_completed_metadata,
    get_records_for_exif_writing
)

from .utils import stop_event, get_date_from_metadata, create_unique_filepath, create_interactive_menu
from .config_editor_logic import get_key

# --- INICJALIZACJA I KONFIGURACJA MODU≈ÅU ---
console = Console(record=True)
logger = logging.getLogger(__name__)

# Definicje sta≈Çych dla plik√≥w log√≥w i postƒôpu
LOG_FILE = Path("app_data/dziennik/advanced_scanner.log")
BATCH_SIZE = 50 # Liczba wynik√≥w zapisywanych do bazy w jednej transakcji


# ##############################################################################
# ===            SEKCJA 1: FUNKCJE POMOCNICZE I NARZƒòDZIA MENU               ===
# ##############################################################################


async def export_fix_needed_urls_to_file():
    """
    Eksportuje do pliku `urls_to_fix.txt` listƒô adres√≥w URL, kt√≥re wymagajƒÖ
    ponownego skanowania w celu uzupe≈Çnienia brakujƒÖcych metadanych.
    """
    FIX_URL_FILE = Path("urls_to_fix.txt")
    console.clear()
    logger.info(f"Rozpoczynam eksport URL-i wymagajƒÖcych naprawy do pliku '{FIX_URL_FILE.name}'...")
    console.print(Panel(f"üì¶ Eksport URL-i do Naprawy do Pliku '{FIX_URL_FILE.name}'", expand=False, style="blue"))

    try:
        urls_to_fix = await get_urls_to_fix()

        if not urls_to_fix:
            logger.info("Nie znaleziono ≈ºadnych wpis√≥w wymagajƒÖcych naprawy metadanych.")
            console.print("\n[bold green]‚úÖ WyglƒÖda na to, ≈ºe wszystkie metadany w bazie sƒÖ kompletne.[/bold green]")
            return

        with open(FIX_URL_FILE, "w", encoding="utf-8") as f:
            for url in urls_to_fix:
                f.write(f"{url}\n")
        
        logger.info(f"Sukces! Wyeksportowano {len(urls_to_fix)} adres√≥w URL do pliku.")
        console.print(f"\n[bold green]‚úÖ Pomy≈õlnie zapisano {len(urls_to_fix)} URL-i w pliku:[/bold green]")
        console.print(f"[cyan]{FIX_URL_FILE.resolve()}[/cyan]")

    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas eksportu URL-i do naprawy: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd. Sprawd≈∫ plik logu, aby uzyskaƒá wiƒôcej informacji.[/bold red]")


async def block_unwanted_resources(route):
    """
    Przechwytuje i opcjonalnie blokuje ≈ºƒÖdania sieciowe strony.
    """
    resource_type = route.request.resource_type
    if resource_type in BLOCKED_RESOURCE_TYPES:
        logger.debug(f"Blokujƒô zas√≥b typu '{resource_type}': {route.request.url[:80]}...")
        await route.abort()
    else:
        await route.continue_()


async def export_urls_from_db_to_file():
    """
    Eksportuje wszystkie adresy URL z tabeli `downloaded_media` w bazie
    danych do pliku tekstowego zdefiniowanego w `config.py`.
    """
    console.clear()
    logger.info(f"Rozpoczynam eksport wszystkich adres√≥w URL do pliku '[bold cyan]{URL_INPUT_FILE}[/bold cyan]'...", extra={"markup": True})
    console.print(Panel(f"üì¶ Eksport Wszystkich URL-i z Bazy do Pliku", expand=False, style="blue"))

    try:
        urls = await get_all_urls_from_db()
        
        if not urls:
            logger.warning("Baza danych jest pusta lub nie zawiera ≈ºadnych adres√≥w URL.")
            console.print("\n[bold yellow]Nie znaleziono ≈ºadnych adres√≥w URL w bazie danych do wyeksportowania.[/bold yellow]")
            return
            
        output_file = Path(URL_INPUT_FILE)
        with open(output_file, "w", encoding="utf-8") as f:
            for url in urls:
                f.write(f"{url}\n")
                
        logger.info(f"Sukces! Wyeksportowano {len(urls)} adres√≥w URL do pliku.")
        console.print(f"\n[bold green]‚úÖ Pomy≈õlnie zapisano {len(urls)} URL-i w pliku:[/bold green]")
        console.print(f"[cyan]{output_file.resolve()}[/cyan]")
        
    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas eksportu adres√≥w URL: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd. Sprawd≈∫ plik logu, aby uzyskaƒá wiƒôcej informacji.[/bold red]")


async def get_urls_for_processing(process_mode: str, input_file: str = URL_INPUT_FILE) -> list[str] | None:
    """
    Przygotowuje listƒô adres√≥w URL do przetworzenia w zale≈ºno≈õci od trybu pracy.
    """
    logger.info(f"Przygotowujƒô listƒô URL-i do przetworzenia w trybie: [bold]{process_mode}[/bold]", extra={"markup": True})

    url_file = Path(input_file)
    if process_mode in ['scan_all', 'scan_fix_file']:
        try:
            if not url_file.exists():
                logger.warning(f"Nie znaleziono pliku wej≈õciowego '{url_file}'.")
                console.print(f"\n[bold yellow]Plik '{url_file}' nie istnieje. U≈ºyj opcji eksportu w menu, aby go utworzyƒá.[/bold yellow]")
                return []
            with open(url_file, "r", encoding="utf-8") as f:
                urls = [line.strip() for line in f if line.strip().startswith("http")]
            if not urls:
                logger.warning(f"Plik '{url_file}' jest pusty lub nie zawiera prawid≈Çowych link√≥w.")
            else:
                logger.info(f"Znaleziono {len(urls)} URL-i w pliku '{url_file.name}'.")
            return urls
        except Exception as e:
            logger.critical(f"B≈ÅƒÑD: Nie mo≈ºna odczytaƒá pliku '{url_file}': {e}", exc_info=True)
            return None

    try:
        scan_type_map = {
            'retry_errors': 'retry_errors', 'force_refresh': 'force_refresh',
            'full_scan': 'new_only'
        }
        scan_type = scan_type_map.get(process_mode, 'new_only')
        
        urls_from_db = await get_urls_for_online_scan(scan_type)
        logger.info(f"Znaleziono {len(urls_from_db)} URL-i w bazie danych pasujƒÖcych do kryteri√≥w.")
        return urls_from_db
    except Exception as e:
        logger.critical(f"B≈ÅƒÑD: Nie mo≈ºna pobraƒá danych z bazy: {e}", exc_info=True)
        return None


def log_to_file(url: str, details: dict, status: str):
    """
    Zapisuje szczeg√≥≈Çowy log pojedynczej operacji do pliku tekstowego.
    """
    try:
        log_path = Path(LOG_FILE)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"--- {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n")
            f.write(f"URL: {url}\n")
            f.write(f"Status: {status}\n")
            if details:
                f.write(json.dumps(details, ensure_ascii=False, indent=4))
            f.write("\n\n")
            
        logger.debug(f"Zapisano wpis dla URL ...{url[-40:]} do pliku logu '{log_path.name}'.")

    except Exception as e:
        logger.error(f"Nie uda≈Ço siƒô zapisaƒá do pliku logu '{LOG_FILE}': {e}", exc_info=True)


# ##############################################################################
# ===            SEKCJA 2: G≈Å√ìWNA LOGIKA SKANERA I KOREKTOR√ìW                ===
# ##############################################################################

async def get_advanced_photo_details_from_page(page: Page, current_url: str) -> dict | None:
    """
    Skaner Online: Pobiera wszystkie zaawansowane metadane ze strony zdjƒôcia
    i oblicza OCZEKIWANƒÑ ≈õcie≈ºkƒô zapisu (`expected_path`).
    """
    # ... (kod tej funkcji pozostaje bez zmian, poniewa≈º nie zawiera zapyta≈Ñ SQL) ...
    months_map = {
        'sty': 1, 'lut': 2, 'mar': 3, 'kwi': 4, 'maj': 5, 'cze': 6,
        'lip': 7, 'sie': 8, 'wrz': 9, 'pa≈∫': 10, 'lis': 11, 'gru': 12
    }
    
    async def get_attribute_safely(locator, attribute='aria-label'):
        """Bezpiecznie pobiera atrybut, aby uniknƒÖƒá b≈Çƒôd√≥w."""
        try:
            return await locator.get_attribute(attribute, timeout=1000)
        except Exception:
            return None

    async def _scan_page_content():
        """Wykonuje pojedynczƒÖ pr√≥bƒô skanowania zawarto≈õci strony."""
        logger.debug(f"Rozpoczynam skanowanie zawarto≈õci strony dla URL: ...{current_url[-40:]}")
        scan_results = {}
        
        photo_id_match = re.search(r'AF1Qip[\w-]+', current_url)
        if not photo_id_match:
            logger.warning(f"Nie uda≈Ço siƒô wyodrƒôbniƒá ID zdjƒôcia z URL: {current_url}")
            return None
        photo_id = photo_id_match.group(0)

        # Krok 1: Otw√≥rz panel boczny (je≈õli jest zamkniƒôty)
        info_panel_selector = f"c-wiz[jslog*='{photo_id}']"
        try:
            if not await page.is_visible(info_panel_selector):
                await page.click(INFO_PANEL_BUTTON_SELECTOR, timeout=WAIT_FOR_SELECTOR * 1000)
                await page.wait_for_selector(info_panel_selector, timeout=WAIT_FOR_SELECTOR * 1000, state="visible")
                logger.debug("Panel boczny zosta≈Ç pomy≈õlnie otwarty.")
        except Exception:
            logger.warning("Nie uda≈Ço siƒô otworzyƒá panelu bocznego. Metadane mogƒÖ byƒá niekompletne.")

        wiz_element = page.locator(info_panel_selector).first

        # Krok 2: Ekstrakcja stabilnych danych
        
        # Data z panelu bocznego i atrybutu aria-label
        date_from_panel, date_from_aria = None, None
        # ... (ta logika pozostaje bez zmian) ...
        date_text_locator = wiz_element.locator(".R9U8ab")
        if await date_text_locator.count() > 0:
            date_text = await date_text_locator.first.inner_text()
            match = re.search(r'(\d{1,2})\s+([a-zA-Z]{3})\s+(\d{4}),\s+(\d{2}:\d{2})', date_text)
            if match:
                day, month_str, year, time_str = match.groups()
                if month := months_map.get(month_str.lower()):
                    hour, minute = map(int, time_str.split(':'))
                    date_from_panel = datetime(int(year), month, int(day), hour, minute)
                    scan_results["DateTime_Panel"] = date_from_panel.isoformat()
                    logger.debug(f"Znaleziono datƒô w panelu bocznym: {date_from_panel.isoformat()}")

        main_image_locator = page.locator("img.BiCYpc[aria-label]")
        if await main_image_locator.count() > 0:
            aria_label = await main_image_locator.first.get_attribute('aria-label')
            if aria_label:
                match_aria = re.search(r'(\d{1,2})\s+([a-zA-Z]{3})\s+(\d{4}),\s+(\d{2}:\d{2}:\d{2})', aria_label, re.IGNORECASE)
                if match_aria:
                    day, month_str, year, time_str = match_aria.groups()
                    if month := months_map.get(month_str.lower()):
                        h, m, s = map(int, time_str.split(':'))
                        date_from_aria = datetime(int(year), month, int(day), h, m, s)
                        scan_results["DateTime_AriaLabel"] = date_from_aria.isoformat()
                        logger.debug(f"Znaleziono datƒô w atrybucie aria-label: {date_from_aria.isoformat()}")

        if date_from_panel:
            scan_results["DateTime"] = date_from_panel.isoformat()
            scan_results["DateTimeSource"] = "Panel Boczny"
        elif date_from_aria:
            scan_results["DateTime"] = date_from_aria.isoformat()
            scan_results["DateTimeSource"] = "Atrybut 'aria-label'"
        
        # Pobieranie pozosta≈Çych metadanych
        pairs = {
            "FileName": "div.R9U8ab[aria-label^='Nazwa pliku:']",
            "Camera": "div.R9U8ab[aria-label^='Nazwa aparatu:']",
            "Location": "div.R9U8ab[aria-label='Lokalizacja']",
            "Dimensions": "span[aria-label^='Rozmiar w pikselach']",
            "FileSize": "span[aria-label^='Rozmiar pliku:']"
        }
        for key, selector in pairs.items():
            locator = wiz_element.locator(selector).first
            if await locator.count() > 0:
                if aria := await get_attribute_safely(locator):
                    value = aria.split(":", 1)[-1].strip() if ':' in aria else aria
                    scan_results[key] = value
                    logger.debug(f"Znaleziono '{key}': {value}")
        
        if description := await wiz_element.locator("textarea[aria-label='Opis']").input_value():
            scan_results["Description"] = description.strip()
            logger.debug(f"Znaleziono opis: '{description[:50]}...'")
        
        people_locators = await wiz_element.locator("a[aria-label^='Na zdjƒôciu:']").all()
        if tagged_people := [await get_attribute_safely(loc) for loc in people_locators]:
            scan_results["TaggedPeople"] = [p.replace("Na zdjƒôciu:", "").strip() for p in tagged_people if p]
            logger.debug(f"Znaleziono osoby: {scan_results['TaggedPeople']}")
        
        albums_section = wiz_element.locator("div.KlIBpb:has-text('Albumy')")
        if await albums_section.count() > 0:
            album_locators = await albums_section.locator("div.AJM7gb").all()
            scan_results["Albums"] = [await loc.inner_text() for loc in album_locators]
            logger.debug(f"Znaleziono albumy: {scan_results['Albums']}")

        # --- Krok 3: Ekstrakcja Danych Eksperymentalnych/Testowych ---
        # Ta sekcja zawiera selektory, kt√≥re sƒÖ mniej stabilne lub s≈Çu≈ºƒÖ
        # do cel√≥w diagnostycznych. Dane z nich sƒÖ dodawane do pod-s≈Çownika
        # 'Experimental_Details', aby nie mieszaƒá ich z g≈Ç√≥wnymi metadanymi.
        
        experimental_details = {}
        
        map_link_locator = wiz_element.locator("a.cFLCHe")
        if await map_link_locator.count() > 0:
            href = await map_link_locator.get_attribute('href')
            if href and (gps_match := re.search(r'(-?\d+\.\d+),(-?\d+\.\d+)', href)):
                experimental_details["GPS_Coords"] = {"latitude": float(gps_match.group(1)), "longitude": float(gps_match.group(2))}
        
        upload_source_locator = wiz_element.locator("div.ffq9nc:has-text('Przes≈Çane z') dd.rCexAf")
        if await upload_source_locator.count() > 0:
            experimental_details["Upload_Source"] = await upload_source_locator.first.inner_text()
            
        album_info_locator = albums_section.locator(".rugHuc").first
        if await album_info_locator.count() > 0:
            album_text = await album_info_locator.inner_text()
            if match := re.search(r'(\d+)\s+element', album_text):
                experimental_details["Album_Element_Count"] = int(match.group(1))

        if experimental_details:
            scan_results["Experimental_Details"] = experimental_details
            logger.debug(f"Zebrano {len(experimental_details)} dodatkowych danych eksperymentalnych.")
            
        return scan_results
    
    # --- G≈Ç√≥wna logika wykonania z mechanizmem ponawiania ---
    try:
        details = await _scan_page_content()

        if details is not None and "DateTime" not in details:
            logger.warning(f"Brak kluczowej daty dla ...{current_url[-40:]}. Od≈õwie≈ºam i pr√≥bujƒô ponownie...", extra={"markup": True})
            await page.reload(wait_until='load', timeout=WAIT_FOR_PAGE_LOAD * 1000)
            await asyncio.sleep(2)
            details = await _scan_page_content()

        if details is None:
            return None

        # Oblicz OCZEKIWANƒÑ ≈õcie≈ºkƒô zapisu na podstawie zebranych metadanych
        if "DateTime" in details and "FileName" in details:
            try:
                dt = datetime.fromisoformat(details["DateTime"])
                dest_dir = Path(DOWNLOADS_DIR_BASE) / str(dt.year) / f"{dt.month:02d}"
                details['expected_path'] = str(dest_dir / details['FileName'])
                logger.debug(f"Obliczono oczekiwanƒÖ ≈õcie≈ºkƒô: {details['expected_path']}")
            except (ValueError, TypeError):
                details['expected_path'] = None
        else:
            details['expected_path'] = None
        
        logger.info(f"Skanowanie online dla ...{current_url[-40:]} zako≈Ñczone pomy≈õlnie.")
        return details

    except Exception as e:
        logger.error(f"Krytyczny b≈ÇƒÖd podczas analizy strony {current_url}: {e}", exc_info=True)
        return None


async def run_scanner_core(process_mode: str, run_headless: bool, input_file: str = URL_INPUT_FILE):
    """
    G≈Ç√≥wna pƒôtla wykonawcza dla skanera dzia≈ÇajƒÖcego w trybie online.
    """
    # ... (kod tej funkcji pozostaje bez zmian, u≈ºywa ju≈º update_scanned_entries_batch) ...
    title_map = {
        'full_scan': "Doka≈Ñczanie Skanowania",
        'retry_errors': "Ponawianie B≈Çƒôd√≥w",
        'force_refresh': "Pe≈Çne Od≈õwie≈ºanie",
        'scan_all': f"Skanowanie z Pliku ({Path(input_file).name})",
        'scan_fix_file': f"Skanowanie z Pliku Naprawczego ({Path(input_file).name})"
    }
    logger.info(f"Uruchamiam Zaawansowany Skaner Online w trybie: [bold cyan]{title_map.get(process_mode)}[/bold cyan]", extra={"markup": True})

    # Krok 1: Pobierz listƒô URL-i do przetworzenia
    urls_to_process = await get_urls_for_processing(process_mode, input_file=input_file)
    if urls_to_process is None: # Krytyczny b≈ÇƒÖd odczytu
        console.print("[bold red]WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas przygotowywania listy URL-i. Sprawd≈∫ logi.[/bold red]")
        return
    if not urls_to_process:
        console.print("\n[bold green]‚úÖ Brak pracy do wykonania dla wybranych kryteri√≥w.[/bold green]")
        logger.info("Brak URL-i do przetworzenia. Ko≈Ñczƒô pracƒô.")
        return
        
    logger.info(f"Znaleziono [bold cyan]{len(urls_to_process)}[/bold cyan] URL-i do przetworzenia. Log: [yellow]{LOG_FILE.name}[/yellow]", extra={"markup": True})

    if process_mode in ['scan_all', 'scan_fix_file']:
        done_file_path = Path(input_file).with_suffix(f"{Path(input_file).suffix}_done")
        console.print(f"[dim]Przetworzone adresy bƒôdƒÖ przenoszone z '{Path(input_file).name}' do '{done_file_path.name}'.[/dim]")

    # Krok 2: Inicjalizacja zasob√≥w (przeglƒÖdarka, interfejs)
    p, browser, results_batch = None, None, []
    try:
        p = await async_playwright().start()
        browser = await getattr(p, BROWSER_TYPE).launch_persistent_context(Path(SESSION_DIR).expanduser(), headless=run_headless, args=BROWSER_ARGS.get(BROWSER_TYPE))
        page = await browser.new_page()
        
        if ENABLE_RESOURCE_BLOCKING:
            logger.info("Blokowanie zbƒôdnych zasob√≥w sieciowych jest [bold green]W≈ÅƒÑCZONE[/bold green].", extra={"markup": True})
            await page.route("**/*", block_unwanted_resources)

        # Inicjalizacja interfejsu Rich.Live
        counters = {"poprawne": 0, "b≈Çƒôdy": 0}
        recent_logs = deque(maxlen=5)
        progress_bar = Progress(TextColumn("[bold blue]{task.description}"), BarColumn(), "[progress.percentage]{task.percentage:>3.1f}%", "{task.completed}/{task.total}", TimeRemainingColumn())
        remaining_urls = list(urls_to_process)
        overall_task = progress_bar.add_task("[green]Postƒôp...", total=len(remaining_urls))
        layout = Layout()
        layout.split_column(Layout(progress_bar, name="progress", size=3), Layout(name="main_body"), Layout(name="footer", size=3))
        
        # Krok 3: G≈Ç√≥wna pƒôtla przetwarzania
        with Live(layout, screen=True, transient=True, auto_refresh=False) as live:
            for url in list(remaining_urls):
                if stop_event.is_set():
                    logger.warning("Przerwanie przez u≈ºytkownika. Zatrzymujƒô skanowanie...")
                    break

                photo_details, final_error = None, "Nieznany b≈ÇƒÖd"
                # Pƒôtla ponawiania pr√≥b dla pojedynczego URL
                for attempt in range(3):
                    if stop_event.is_set(): break
                    try:
                        await page.goto(url, wait_until='load', timeout=WAIT_FOR_PAGE_LOAD * 1000)
                        photo_details = await get_advanced_photo_details_from_page(page, url)
                        if photo_details: break # Sukces, przerywamy pƒôtlƒô ponawiania
                    except Exception as e:
                        final_error = str(e)
                        logger.warning(f"B≈ÇƒÖd podczas pr√≥by {attempt + 1} dla ...{url[-30:]}: {e}")
                
                if stop_event.is_set(): break
                
                # Przetwarzanie wyniku i aktualizacja UI
                if photo_details:
                    status = "Sukces"
                    counters["poprawne"] += 1
                    result_table = Table(show_header=False, box=None, padding=0, expand=True)
                    result_table.add_column(style="cyan", justify="right", width=25)
                    result_table.add_column()
                    for key, value in photo_details.items():
                        if isinstance(value, list): result_table.add_row(f"{key}:", "\n".join(f"- {item}" for item in value))
                        elif isinstance(value, dict): result_table.add_row(f"{key}:", json.dumps(value, indent=2, ensure_ascii=False))
                        else: result_table.add_row(f"{key}:", str(value))
                    recent_logs.appendleft(Panel(result_table, title=f"[bold green]‚úÖ Sukces![/] ...{url[-30:]}"))
                else:
                    status = "B≈ÇƒÖd"
                    photo_details = {"error": final_error}
                    counters["b≈Çƒôdy"] += 1
                    recent_logs.appendleft(Panel(f"Nie uda≈Ço siƒô pobraƒá danych. Ostatni b≈ÇƒÖd: {final_error[:100]}...", title=f"[bold red]‚ùå B≈ÇƒÖd![/] ...{url[-30:]}", border_style="red"))
                
                # Dodaj wynik do partii do zapisu w bazie
                results_batch.append({
                    "url": url,
                    "metadata_json": json.dumps(photo_details, ensure_ascii=False) if photo_details else None,
                    "processing_status": status,
                    "expected_path": photo_details.get('expected_path') if photo_details else None
                })
                log_to_file(url, photo_details, status)
                
                # Zapisz partiƒô do bazy, je≈õli osiƒÖgnƒô≈Ça odpowiedni rozmiar
                if len(results_batch) >= BATCH_SIZE:
                    await update_scanned_entries_batch(results_batch)
                    results_batch.clear()

                # Aktualizacja plik√≥w postƒôpu w trybie skanowania z pliku
                if process_mode in ['scan_all', 'scan_fix_file']:
                    try:
                        remaining_urls.remove(url)
                        with open(done_file_path, "a", encoding="utf-8") as f_done: f_done.write(f"{url}\n")
                        with open(input_file, "w", encoding="utf-8") as f_input:
                            for rem_url in remaining_urls: f_input.write(f"{rem_url}\n")
                    except (IOError, ValueError) as e:
                        logger.error(f"B≈ÇƒÖd podczas aktualizacji plik√≥w postƒôpu: {e}")
                
                # Aktualizacja interfejsu
                progress_bar.update(overall_task, advance=1)
                layout["main_body"].update(Panel(Group(*recent_logs), title="Ostatnie Akcje"))
                counters_table = Table.grid(expand=True); counters_table.add_column(justify="center"); counters_table.add_column(justify="center")
                counters_table.add_row(f"[green]Poprawne: {counters['poprawne']}[/]", f"[red]B≈Çƒôdy: {counters['b≈Çƒôdy']}[/]")
                layout["footer"].update(Panel(counters_table, title="Statystyki Sesji"))
                live.refresh()

        # Zapisz ostatniƒÖ partiƒô danych, je≈õli jaka≈õ zosta≈Ça
        if results_batch:
            await update_scanned_entries_batch(results_batch)
            
    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç nieobs≈Çugiwany b≈ÇƒÖd w g≈Ç√≥wnej pƒôtli skanera: {e}", exc_info=True)
    finally:
        logger.info("Zamykanie zasob√≥w skanera online...")
        if browser: await browser.close()
        if p: await p.stop()
        logger.info("Zasoby skanera zwolnione.")

# plik: core/advanced_scanner_logic.py

async def run_offline_file_corrector():
    """
    Skaner Offline z SamokorektƒÖ Lokalizacji Plik√≥w.
    """
    console.clear()
    logger.info("Uruchamiam Skaner Offline z SamokorektƒÖ Lokalizacji Plik√≥w...")
    console.print(Panel("üõ∞Ô∏è  Korektor Lokalizacji Plik√≥w (Offline) üõ∞Ô∏è", expand=False, style="green"))
    
    try:
        # Krok 1: Pobierz dane do weryfikacji za pomocƒÖ nowej, dedykowanej funkcji
        records_to_check = await get_records_for_path_correction()
        
        if not records_to_check:
            logger.warning("Nie znaleziono plik√≥w do weryfikacji.")
            console.print("\n[bold yellow]Nie znaleziono plik√≥w do weryfikacji. Uruchom skaner online.[/bold yellow]")
            return

        logger.info(f"Znaleziono {len(records_to_check)} plik√≥w do weryfikacji lokalizacji.")
        moved_count, error_count, skipped_count = 0, 0, 0
        
        # Krok 2: Iteruj i naprawiaj niesp√≥jno≈õci
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[green]Weryfikacja lokalizacji plik√≥w...", total=len(records_to_check))
            for record in records_to_check:
                try:
                    # U≈ºywamy pathlib.Path do obs≈Çugi ≈õcie≈ºek
                    final_path = Path(record['final_path'])
                    expected_path = Path(record['expected_path'])

                    if not final_path.name or not expected_path.name:
                        logger.warning(f"Pominiƒôto rekord ID {record['id']} z powodu nieprawid≈Çowej ≈õcie≈ºki.")
                        skipped_count += 1
                        continue

                    # Por√≥wnujemy rozwiƒÖzane, absolutne ≈õcie≈ºki
                    if final_path.resolve() != expected_path.resolve():
                        console.print(f"\n[yellow]Niesp√≥jno≈õƒá wykryta dla ID {record['id']}:[/]")
                        console.print(f"  [dim]Jest w:[/dim] {final_path}")
                        console.print(f"  [cyan]Powinien byƒá w:[/cyan] {expected_path}")

                        if not await asyncio.to_thread(final_path.exists):
                            logger.error(f"B≈ÅƒÑD: Plik ≈∫r√≥d≈Çowy {final_path} nie istnieje. Pomijam.")
                            error_count += 1
                            continue

                        await asyncio.to_thread(expected_path.parent.mkdir, parents=True, exist_ok=True)
                        await asyncio.to_thread(shutil.move, str(final_path), str(expected_path))
                        
                        # Zaktualizuj wpis w bazie za pomocƒÖ nowej funkcji
                        await update_final_path(record['id'], str(expected_path))
                        
                        console.print(f"  [bold green]Sukces: Plik zosta≈Ç przeniesiony.[/bold green]")
                        moved_count += 1
                except Exception as e:
                    logger.error(f"B≈ÅƒÑD podczas przenoszenia pliku dla ID {record['id']}: {e}", exc_info=True)
                    error_count += 1
                finally:
                    progress.update(task, advance=1)

        # Krok 3: Wy≈õwietl podsumowanie
        logger.info("Zako≈Ñczono weryfikacjƒô lokalizacji plik√≥w.")
        console.print("\n[bold green]Zako≈Ñczono weryfikacjƒô lokalizacji plik√≥w.[/bold green]")
        console.print(f"  - Przeniesiono plik√≥w: [cyan]{moved_count}[/cyan]")
        console.print(f"  - Pominiƒôto (b≈Çƒôdne dane): [yellow]{skipped_count}[/yellow]")
        console.print(f"  - B≈Çƒôdy: [red]{error_count}[/red]")

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny w korektorze plik√≥w: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd krytyczny. Sprawd≈∫ logi.[/bold red]")


async def run_filename_fixer_from_db():
    """
    Skaner Offline: Naprawia nazwy plik√≥w na dysku na podstawie metadanych.
    """
    console.clear()
    logger.info("Uruchamiam narzƒôdzie do naprawy nazw plik√≥w na podstawie metadanych...")
    console.print(Panel("[bold yellow]Naprawa Nazw Plik√≥w z Pe≈ÇnƒÖ SynchronizacjƒÖ[/]", expand=False))
    
    if not Confirm.ask("\n[bold red]UWAGA:[/bold red] Ta operacja zmieni nazwy plik√≥w na Twoim dysku. Czy na pewno chcesz kontynuowaƒá?", default=False):
        logger.warning("Naprawa nazw plik√≥w anulowana przez u≈ºytkownika.")
        return

    try:
        run_count = 0
        while True:
            run_count += 1
            logger.info(f"Rozpoczynam przebieg {run_count} weryfikacji nazw plik√≥w.")
            
            records_to_check = await get_records_for_filename_fix()
            
            if not records_to_check:
                logger.info("Nie znaleziono plik√≥w z metadanymi do weryfikacji nazw.")
                break

            mismatches_found_this_run = False
            with Progress(console=console, transient=True) as progress:
                task = progress.add_task(f"[green]Weryfikacja nazw (przebieg {run_count})...[/]", total=len(records_to_check))
                for record in records_to_check:
                    progress.update(task, advance=1)
                    try:
                        current_path = Path(record['final_path'])
                        metadata = json.loads(record['metadata_json'])
                        filename_from_meta = metadata.get('FileName')
                        
                        if not await asyncio.to_thread(current_path.exists):
                            continue

                        mismatches_found_this_run = True
                        
                        new_path = create_unique_filepath(current_path.parent, filename_from_meta)
                        new_filename = new_path.name

                        console.print(f"\n[yellow]Niesp√≥jno≈õƒá nazwy dla ID {record['id']}:[/]")
                        console.print(f"  [dim]Aktualna nazwa:[/dim] {current_path.name}")
                        console.print(f"  [cyan]Oczekiwana nazwa:[/cyan] {filename_from_meta}")
                        if new_filename != filename_from_meta:
                            console.print(f"  [magenta]Kolizja! Zmieniam nazwƒô na:[/magenta] {new_filename}")

                        await asyncio.to_thread(current_path.rename, new_path)
                        
                        if new_filename != filename_from_meta:
                            metadata['FileName'] = new_filename
                        
                        if 'DateTime' in metadata:
                            dt = datetime.fromisoformat(metadata["DateTime"])
                            dest_dir = Path(DOWNLOADS_DIR_BASE) / str(dt.year) / f"{dt.month:02d}"
                            new_expected_path = str(dest_dir / new_filename)
                            metadata['expected_path'] = new_expected_path
                        else:
                            new_expected_path = str(new_path)

                        await update_entry_after_rename(
                            record['id'], new_filename, str(new_path),
                            new_expected_path, json.dumps(metadata, ensure_ascii=False)
                        )
                        
                        logger.info(f"Zsynchronizowano plik ID {record['id']}. Nowa nazwa: '{new_filename}'.")
                        console.print("  [bold green]Sukces: Plik i wpis w bazie zosta≈Çy w pe≈Çni zsynchronizowane.[/bold green]")
                    
                    except Exception as e:
                        logger.error(f"B≈ÇƒÖd podczas naprawy nazwy dla pliku {record['final_path']}", exc_info=True)
            
            if not mismatches_found_this_run:
                logger.info("Brak dalszych niesp√≥jno≈õci. Ko≈Ñczƒô pƒôtlƒô naprawczƒÖ.")
                break
                
        logger.info("Zako≈Ñczono naprawƒô nazw plik√≥w.")
        console.print("\n[bold green]Zako≈Ñczono. Wszystkie nazwy plik√≥w sƒÖ teraz sp√≥jne z metadanymi.[/bold green]")
        
    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny podczas naprawy nazw plik√≥w: {e}", exc_info=True)


async def run_metadata_completer():
    """
    Skaner Offline: Uzupe≈Çnia brakujƒÖce dane i oblicza `expected_path`.
    """
    console.clear()
    logger.info("Uruchamiam narzƒôdzie do uzupe≈Çniania metadanych...")
    console.print(Panel("[bold blue]Uzupe≈Çniacz Danych i Oczekiwanych ≈öcie≈ºek[/]", expand=False))
    
    if not EXIFTOOL_AVAILABLE:
        # ... (obs≈Çuga braku exiftool bez zmian) ...
        return

    try:
        records_to_fix = await get_records_for_metadata_completion()
        
        if not records_to_fix:
            logger.info("Nie znaleziono plik√≥w wymagajƒÖcych uzupe≈Çnienia danych.")
            console.print("\n[bold green]‚úÖ Wszystkie pobrane pliki majƒÖ ju≈º obliczonƒÖ oczekiwanƒÖ ≈õcie≈ºkƒô.[/bold green]")
            return

        logger.info(f"Znaleziono {len(records_to_fix)} plik√≥w do uzupe≈Çnienia danych.")
        if not Confirm.ask("\n[cyan]Czy chcesz kontynuowaƒá?[/]", default=True):
            logger.warning("Operacja uzupe≈Çniania danych anulowana."); return

        fixed_count, error_count = 0, 0
        loop = asyncio.get_running_loop()
        
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[green]Uzupe≈Çnianie danych...", total=len(records_to_fix))
            for record in records_to_fix:
                try:
                    current_path = Path(record['final_path'])
                    if not await asyncio.to_thread(current_path.exists):
                        logger.warning(f"Plik {current_path} nie istnieje. Pomijam.")
                        continue

                    existing_metadata = json.loads(record['metadata_json'])
                    
                    with exiftool.ExifToolHelper() as et:
                        exif_metadata_list = await loop.run_in_executor(None, et.get_metadata, str(current_path))
                    
                    if not exif_metadata_list:
                         logger.warning(f"Nie odczytano EXIF dla {current_path.name}."); continue

                    merged_metadata = exif_metadata_list[0]
                    merged_metadata.update(existing_metadata)

                    if 'DateTime' not in merged_metadata or not merged_metadata['DateTime']:
                        date_obj = await get_date_from_metadata(merged_metadata)
                        if date_obj: merged_metadata['DateTime'] = date_obj.isoformat()
                    
                    if 'FileName' not in merged_metadata or not merged_metadata['FileName']:
                         merged_metadata['FileName'] = merged_metadata.get('File:FileName', current_path.name)
                         
                    if 'DateTime' in merged_metadata and 'FileName' in merged_metadata:
                        dt = datetime.fromisoformat(merged_metadata["DateTime"])
                        dest_dir = Path(DOWNLOADS_DIR_BASE) / str(dt.year) / f"{dt.month:02d}"
                        expected_path = str(dest_dir / merged_metadata['FileName'])
                        
                        await update_entry_with_completed_metadata(
                            record['id'], json.dumps(merged_metadata, ensure_ascii=False), expected_path
                        )
                        fixed_count += 1
                    else:
                        logger.error(f"Nie uda≈Ço siƒô ustaliƒá daty/nazwy dla ID {record['id']}.")
                        error_count += 1

                except Exception as e:
                    logger.error(f"B≈ÇƒÖd przetwarzania pliku {record['final_path']}", exc_info=True)
                    error_count += 1
                finally:
                    progress.update(task, advance=1)
                        
        logger.info("Zako≈Ñczono uzupe≈Çnianie danych.")
        console.print(f"\n[bold green]Zako≈Ñczono. Uzupe≈Çniono dane dla [cyan]{fixed_count}[/cyan] plik√≥w. B≈Çƒôdy: [red]{error_count}[/red].[/bold green]")

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny podczas uzupe≈Çniania metadanych: {e}", exc_info=True)


# ##############################################################################
# ===                  SEKCJA 3: NARZƒòDZIA ZARZƒÑDCZE I DIAGNOSTYCZNE         ===
# ##############################################################################

async def write_metadata_from_db_to_files():
    """
    Odczytuje metadane z bazy danych i zapisuje je do plik√≥w na dysku.
    """
    console.clear()
    logger.info("Uruchamiam narzƒôdzie do zapisu metadanych w plikach (Exiftool)...")
    console.print(Panel("‚úçÔ∏è Zapisywanie Metadanych z Bazy do Plik√≥w (Exiftool)", expand=False, style="red"))
    
    if not EXIFTOOL_AVAILABLE:
        # ... (obs≈Çuga braku exiftool bez zmian) ...
        return
        
    console.print("\n[bold yellow]‚ö†Ô∏è UWAGA: Ta operacja nieodwracalnie zmodyfikuje pliki na dysku![/bold yellow]")
    if not Confirm.ask("Czy na pewno chcesz kontynuowaƒá? (Zalecana jest kopia zapasowa)", default=False):
        logger.warning("Operacja zapisu metadanych anulowana przez u≈ºytkownika.")
        return

    records_to_process = await get_records_for_exif_writing()

    if not records_to_process:
        logger.warning("Nie znaleziono plik√≥w z metadanymi do zapisu.")
        console.print("\n[bold yellow]Nie znaleziono plik√≥w z metadanymi do zapisu.[/bold yellow]")
        return

    success_count, error_count, skipped_count = 0, 0, 0
    loop = asyncio.get_running_loop()

    with Progress(console=console, transient=True) as progress:
        task = progress.add_task("[green]Zapisywanie tag√≥w w plikach...", total=len(records_to_process))
        for record in records_to_process:
            try:
                file_path = Path(record['final_path'])
                if not await asyncio.to_thread(file_path.exists):
                    logger.warning(f"Pominiƒôto: Plik nie istnieje {file_path}")
                    skipped_count += 1
                    continue

                data = json.loads(record['metadata_json'])
                
                tags_to_write = {}
                # ... (logika budowania tag√≥w bez zmian) ...
                
                if not tags_to_write:
                    skipped_count += 1; continue

                params = []
                # ... (logika budowania parametr√≥w bez zmian) ...
                
                with exiftool.ExifToolHelper() as et:
                    await loop.run_in_executor(None, et.execute, "-overwrite_original", "-m", *params, str(file_path))
                
                logger.debug(f"Pomy≈õlnie zapisano {len(tags_to_write)} tag√≥w do pliku {file_path.name}")
                success_count += 1

            except json.JSONDecodeError:
                logger.error(f"B≈ÇƒÖd: Uszkodzony JSON dla pliku {record['final_path']}")
                error_count += 1
            except Exception as e:
                logger.error(f"B≈ÇƒÖd zapisu do pliku {record['final_path']}: {e}", exc_info=True)
                error_count += 1
            finally:
                progress.update(task, advance=1)

    logger.info("Zako≈Ñczono zapisywanie metadanych do plik√≥w.")
    console.print("\n[bold green]Zako≈Ñczono zapisywanie metadanych do plik√≥w.[/bold green]")
    console.print(f"  - Zapisano pomy≈õlnie: [cyan]{success_count}[/cyan]")
    console.print(f"  - Pominiƒôto (brak danych/pliku): [yellow]{skipped_count}[/yellow]")
    console.print(f"  - B≈Çƒôdy: [red]{error_count}[/red]")


async def test_single_url_diagnostics(run_headless: bool):
    """
    Uruchamia pe≈Çny test diagnostyczny dla jednego, rƒôcznie podanego adresu URL.
    """
    # ... (kod tej funkcji pozostaje bez zmian) ...
    console.clear()
    console.print(Panel("[bold yellow]üî¨ Test Skanera Online dla Pojedynczego URL üî¨[/]", expand=False))
    url = Prompt.ask("\n[cyan]Wklej adres URL zdjƒôcia, kt√≥ry chcesz przetestowaƒá[/]")
    if not url.strip().startswith("http"):
        logger.error("To nie jest prawid≈Çowy adres URL.")
        return

    logger.info(f"Uruchamianie przeglƒÖdarki w trybie testowym (headless: {run_headless})...")
    
    async with async_playwright() as p:
        browser = None
        try:
            browser = await getattr(p, BROWSER_TYPE).launch_persistent_context(
                Path(SESSION_DIR).expanduser(), headless=run_headless, args=BROWSER_ARGS.get(BROWSER_TYPE)
            )
            page = await browser.new_page()

            if ENABLE_RESOURCE_BLOCKING:
                logger.info("Blokowanie zasob√≥w w≈ÇƒÖczone na czas testu.")
                await page.route("**/*", block_unwanted_resources)

            with console.status(f"[cyan]Nawigacja do: [dim]{url}[/dim]...[/]"):
                await page.goto(url, wait_until='load', timeout=WAIT_FOR_PAGE_LOAD * 1000)
            
            logger.info("Strona za≈Çadowana. Uruchamiam skaner g≈Ç√≥wny...")
            with console.status("[cyan]Skanowanie metadanych ze strony...[/]"):
                metadata = await get_advanced_photo_details_from_page(page, url)
            
            console.clear()
            if metadata:
                console.print(Panel("[bold green]‚úÖ SKANER ZAKO≈ÉCZY≈Å PRACƒò SUKCESEM[/]", title="Wynik Testu"))
                table = Table(title="Zebrane Metadane", show_header=False, box=None, padding=(0, 2))
                table.add_column(style="cyan", justify="right", width=25)
                table.add_column()
                for key, value in metadata.items():
                    if isinstance(value, list):
                        table.add_row(f"[bold]{key}:[/bold]", "\n".join(f"- {item}" for item in value))
                    elif isinstance(value, dict):
                        table.add_row(f"[bold]{key}:[/bold]", json.dumps(value, indent=2, ensure_ascii=False))
                    else:
                        table.add_row(f"[bold]{key}:[/bold]", str(value))
                console.print(Panel(table, border_style="green"))
            else:
                console.print(Panel("[bold red]‚ùå SKANER ZAKO≈ÉCZY≈Å PRACƒò B≈ÅƒòDEM LUB NIE ZNALAZ≈Å DANYCH[/]", title="Wynik Testu", border_style="red"))
                logger.error("Skaner nie zwr√≥ci≈Ç ≈ºadnych metadanych.")

        except Exception as e:
            logger.critical(f"WystƒÖpi≈Ç krytyczny b≈ÇƒÖd podczas testu: {e}", exc_info=True)
            console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd krytyczny. Sprawd≈∫ logi.[/bold red]")
        finally:
            if browser:
                await browser.close()
            logger.info("Test zako≈Ñczony, przeglƒÖdarka zamkniƒôta.")


# ##############################################################################
# ===                    SEKCJA 4: G≈Å√ìWNA FUNKCJA URUCHOMIENIOWA             ===
# ##############################################################################

async def run_advanced_scanner():
    """
    Wy≈õwietla i zarzƒÖdza interaktywnym menu dla wszystkich funkcji
    dostƒôpnych w module Zaawansowanego Skanera i Mened≈ºera Kolekcji.
    """
    logger.info("Uruchamiam menu Zaawansowanego Skanera i Mened≈ºera Kolekcji.")
    
    await setup_database()

    while True:
        console.clear()
        
        menu_items = [
            ("--- G≈Å√ìWNY PRZEP≈ÅYW PRACY (Zalecana kolejno≈õƒá) ---", None),
            ("Krok 1: Doko≈Ñcz skanowanie metadanych z bazy (Online)", "full_scan"),
            ("Krok 2: Uzupe≈Çnij dane i ≈õcie≈ºki z plik√≥w (Offline)", "complete_metadata"),
            ("Krok 3: Sprawd≈∫ i napraw LOKALIZACJE plik√≥w (Offline)", "correct_paths"),
            ("Krok 4: Sprawd≈∫ i napraw NAZWY plik√≥w (Offline)", "fix_filenames"),

            ("--- ZAAWANSOWANE OPERACJE ONLINE ---", None),
            ("Pon√≥w tylko te URL-e z bazy, kt√≥re mia≈Çy b≈ÇƒÖd", "retry_errors"),
            ("Od≈õwie≈º metadane dla WSZYSTKICH wpis√≥w w bazie", "force_refresh"),
            ("Skanuj wszystkie URL-e z pliku 'urls_to_scan.txt'", "scan_all"),
            ("Skanuj URL-e wymagajƒÖce naprawy z pliku 'urls_to_fix.txt'", "scan_fix_file"),

            ("--- NARZƒòDZIA POMOCNICZE I DIAGNOSTYKA ---", None),
            ("ZAPISZ metadane z bazy do plik√≥w (Exiftool)", "write_to_files"),
            ("Wygeneruj plik 'urls_to_scan.txt' z bazy", "export_urls"),
            ("Wygeneruj plik 'urls_to_fix.txt' (z brakujƒÖcymi metadanymi)", "export_fix_urls"),
            ("Uruchom PE≈ÅNY TEST (Skaner + Diagnostyka)", "single_url_test"),
            
            ("---", None),
            ("Wr√≥ƒá do menu g≈Ç√≥wnego", "exit")
        ]

        selected_mode = await create_interactive_menu(
            menu_items,
            "Zaawansowany Skaner i Mened≈ºer Kolekcji",
            border_style="magenta"
        )
        
        if selected_mode == "exit" or selected_mode is None:
            logger.info("Anulowano. Powr√≥t do menu g≈Ç√≥wnego.")
            break
        
        logger.info(f"U≈ºytkownik wybra≈Ç opcjƒô: '{selected_mode}'")
        
        online_modes = ['full_scan', 'retry_errors', 'force_refresh', 'scan_all', 'scan_fix_file']
        
        if selected_mode in online_modes:
            input_file_path = "urls_to_fix.txt" if selected_mode == 'scan_fix_file' else URL_INPUT_FILE
            run_headless = Confirm.ask("Uruchomiƒá w trybie niewidocznym (headless)?", default=DEFAULT_HEADLESS_MODE)
            await run_scanner_core(process_mode=selected_mode, run_headless=run_headless, input_file=input_file_path)
        elif selected_mode == 'correct_paths':
            await run_offline_file_corrector()
        elif selected_mode == 'fix_filenames':
            await run_filename_fixer_from_db()
        elif selected_mode == 'complete_metadata':
            await run_metadata_completer()
        elif selected_mode == 'write_to_files':
            if not EXIFTOOL_AVAILABLE:
                console.print(Panel("[bold red]B≈ÇƒÖd: Brak 'pyexiftool'![/bold red]\nUruchom: [cyan]pip install pyexiftool[/cyan]", title="Instrukcja Instalacji"))
            else:
                await write_metadata_from_db_to_files()
        elif selected_mode == 'export_urls':
            await export_urls_from_db_to_file()
        elif selected_mode == 'export_fix_urls':
            await export_fix_needed_urls_to_file()
        elif selected_mode == 'single_url_test':
            await test_single_url_diagnostics(run_headless=False)
        
        Prompt.ask("\n[bold]Operacja zako≈Ñczona. Naci≈õnij Enter, aby wr√≥ciƒá do menu skanera...[/]", console=console)
