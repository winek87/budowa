# plik: core/integrity_validator_logic.py
# Wersja 9.1 - Scentralizowana logika bazy danych i ujednolicone ≈õcie≈ºki (Refaktoryzacja Fazy 1)

# -*- coding: utf-8 -*-

# plik: core/integrity_validator_logic.py
# Wersja 9.0 - Pe≈Çna integracja z asynchronicznym modu≈Çem bazy danych
#
# ##############################################################################
# ===                        MODU≈Å WALIDATORA INTEGRALNO≈öCI                  ===
# ##############################################################################
#
# "Walidator Integralno≈õci" to zaawansowany zestaw narzƒôdzi do "sprzƒÖtania"
# i weryfikacji pobranej kolekcji. Pozwala na:
#
#  - Obliczanie i zapisywanie sum kontrolnych (hash MD5) dla plik√≥w.
#  - Wyszukiwanie "duch√≥w" (wpis√≥w w bazie bez pliku na dysku) i "sierot"
#    (plik√≥w na dysku bez wpisu w bazie).
#  - Analizƒô sp√≥jno≈õci miƒôdzy datƒÖ w metadanych a lokalizacjƒÖ pliku.
#  - Interaktywne zarzƒÖdzanie duplikatami plik√≥w na podstawie ich zawarto≈õci.
#
################################################################################

# --- G≈Å√ìWNE IMPORTY ---
import asyncio
import hashlib
import os
import json
import logging
import uuid
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from datetime import datetime
from urllib.parse import unquote, urlparse

# --- Zale≈ºno≈õci zewnƒôtrzne (opcjonalne) ---
try:
    import exiftool
    EXIFTOOL_AVAILABLE = True
except ImportError:
    EXIFTOOL_AVAILABLE = False

# --- Importy asynchroniczne ---
import aiosqlite

# --- IMPORTY Z BIBLIOTEKI `rich` ---
from rich.console import Console, Group
from rich.panel import Panel
from rich.text import Text
from rich.live import Live
from rich.prompt import Prompt, Confirm
from rich.table import Table
from rich.progress import Progress
from rich.align import Align
from rich.layout import Layout

# --- IMPORTY Z W≈ÅASNYCH MODU≈Å√ìW ---
from .config import DATABASE_FILE, DOWNLOADS_DIR_BASE, LOCAL_SCANNER_DIRECTORIES
from .utils import create_interactive_menu, _interactive_file_selector, get_key, _parse_metadata_for_display

# NOWE, SCENTRALIZOWANE IMPORTY Z MODU≈ÅU BAZY DANYCH
from .database import (
    add_local_file_entry,
    get_downloaded_files_for_validation,
    get_records_to_hash,
    update_hashes_batch,
    get_all_final_paths,
    delete_entries_by_ids,
    get_metadata_for_consistency_check,
    get_duplicate_hashes,
    get_entries_by_hash,
    get_local_import_entries,
    update_paths_for_entry
)

# --- Inicjalizacja i Konfiguracja Modu≈Çu ---
console = Console()
logger = logging.getLogger(__name__)


# ##############################################################################
# ===           SEKCJA 1: FUNKCJE POMOCNICZE (ASYNC & EFFICIENT)             ===
# ##############################################################################

async def _get_all_disk_paths(root_dir: Path) -> Set[Path]:
    """Asynchronicznie i wydajnie skanuje podany folder w poszukiwaniu plik√≥w."""
    disk_paths: Set[Path] = set()
    logger.info(f"Rozpoczynam asynchroniczne skanowanie dysku w folderze: {root_dir}")
    
    loop = asyncio.get_running_loop()
    
    def scan_directory():
        logger.debug("Uruchamiam blokujƒÖcƒÖ operacjƒô rglob w osobnym wƒÖtku...")
        for path in root_dir.rglob('*'):
            if path.is_file():
                disk_paths.add(path.resolve())
    
    await loop.run_in_executor(None, scan_directory)
    
    logger.info(f"Zako≈Ñczono skanowanie dysku. Znaleziono {len(disk_paths)} plik√≥w.")
    return disk_paths


async def _resolve_duplicates_interactively(duplicate_set: List[Dict]) -> Dict:
    """
    Wy≈õwietla interfejs do rozwiƒÖzania pojedynczego zestawu duplikat√≥w,
    korzystajƒÖc z uniwersalnego komponentu UI.
    """
    from .utils import create_side_by_side_comparison_panel # Importujemy nasz nowy komponent

    selected_to_keep_index = 0
    try:
        # Prosta logika do wstƒôpnego wyboru wiƒôkszego pliku
        size_a_str = duplicate_set[0].get('size', '0 MB').split(' ')[0].replace(',', '.')
        size_b_str = duplicate_set[1].get('size', '0 MB').split(' ')[0].replace(',', '.')
        if float(size_b_str) > float(size_a_str):
            selected_to_keep_index = 1
    except (ValueError, IndexError):
        pass

    def generate_main_layout() -> Layout:
        """Wewnƒôtrzna funkcja renderujƒÖca ca≈Çy widok, w≈ÇƒÖczajƒÖc nag≈Ç√≥wek i stopkƒô."""

        # Krok 1: Przygotuj dane dla uniwersalnego komponentu
        item_a_details = {
            "ID w Bazie": duplicate_set[0].get('id', 'Brak'),
            "≈öcie≈ºka": duplicate_set[0].get('relative_path', 'Brak'),
            "separator_1": "",
            "Data": duplicate_set[0].get('date', 'Brak'),
            "Rozmiar": duplicate_set[0].get('size', 'Brak'),
            "Wymiary": duplicate_set[0].get('dimensions', 'Brak'),
            "separator_2": "",
            "Aparat": duplicate_set[0].get('camera', 'Brak'),
        }
        item_b_details = {
            "ID w Bazie": duplicate_set[1].get('id', 'Brak'),
            "≈öcie≈ºka": duplicate_set[1].get('relative_path', 'Brak'),
            "separator_1": "",
            "Data": duplicate_set[1].get('date', 'Brak'),
            "Rozmiar": duplicate_set[1].get('size', 'Brak'),
            "Wymiary": duplicate_set[1].get('dimensions', 'Brak'),
            "separator_2": "",
            "Aparat": duplicate_set[1].get('camera', 'Brak'),
        }

        # Krok 2: Wywo≈Çaj uniwersalny komponent, aby wygenerowa≈Ç panel por√≥wnawczy
        comparison_panel = create_side_by_side_comparison_panel(
            item_a_details,
            item_b_details,
            is_a_selected=(selected_to_keep_index == 0)
        )

        # Krok 3: Dodaj nag≈Ç√≥wek i stopkƒô
        title_text = f"Wybierz plik do ZACHOWANIA\n[dim]Hash: {duplicate_set[0].get('hash', 'Brak')}[/dim]"
        footer = Align.center(Text("[bold]L/P[/](wybierz)‚Ä¢[bold]ENTER[/](zatwierd≈∫)‚Ä¢[bold]P[/](pomi≈Ñ)‚Ä¢[bold]Q[/](zako≈Ñcz)"))
        main_layout = Layout()
        main_layout.split_column(
            Layout(Align.center(Text(title_text)), size=3),
            comparison_panel,
            Layout(footer, size=1)
        )
        return main_layout

    with Live(generate_main_layout(), screen=True, auto_refresh=False, transient=True) as live:
        while True:
            live.update(generate_main_layout(), refresh=True)
            key = await asyncio.to_thread(get_key)
            if not key: continue

            if key.upper() in ["Q", "ESC"]: return {"action": "quit"}
            if key.upper() == "P": return {"action": "skip"}
            if key in ["LEFT", "RIGHT"]: selected_to_keep_index = 1 - selected_to_keep_index
            if key == "ENTER":
                to_keep = duplicate_set[selected_to_keep_index]
                to_delete = [duplicate_set[1 - selected_to_keep_index]]
                return {"action": "resolve", "keep": to_keep, "delete": to_delete}

# ##############################################################################
# ===                    SEKCJA 2: G≈Å√ìWNE FUNKCJE WALIDATORA                 ===
# ##############################################################################

async def verify_file_existence():
    """Weryfikuje, czy pliki z bazy danych istniejƒÖ na dysku."""
    console.clear()
    logger.info("Uruchamiam Weryfikator Istnienia Plik√≥w...")
    console.print(Panel("üëª Weryfikator Istnienia Plik√≥w ('Duchy' w bazie) üëª", expand=False, style="bold yellow"))

    try:
        records_to_check = await get_downloaded_files_for_validation()

        if not records_to_check:
            logger.warning("Nie znaleziono plik√≥w do weryfikacji.")
            console.print("\n[green]Nie znaleziono w bazie ≈ºadnych plik√≥w ze statusem 'downloaded' do weryfikacji.[/green]")
            return

        missing_files = []
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[green]Sprawdzanie plik√≥w na dysku...", total=len(records_to_check))
            for record in records_to_check:
                file_path = Path(record['final_path'])
                if not await asyncio.to_thread(file_path.exists):
                    missing_files.append(record)
                    logger.warning(f"Brak pliku na dysku dla ID={record['id']}: {file_path}")
                progress.update(task, advance=1)

        if not missing_files:
            logger.info("Weryfikacja zako≈Ñczona pomy≈õlnie.")
            console.print(f"\n[bold green]‚úÖ Weryfikacja zako≈Ñczona. Wszystkie {len(records_to_check)} pliki z bazy danych istniejƒÖ na dysku.[/bold green]")
        else:
            logger.error(f"Znaleziono {len(missing_files)} brakujƒÖcych plik√≥w ('duch√≥w')!")
            console.print(f"\n[bold red]‚ö†Ô∏è Znaleziono {len(missing_files)} brakujƒÖcych plik√≥w:[/bold red]")
            
            table = Table(title="Lista BrakujƒÖcych Plik√≥w ('Duchy')")
            table.add_column("ID Wpisu", style="cyan", justify="right")
            table.add_column("Oczekiwana ≈öcie≈ºka", style="red")
            for missing in missing_files:
                table.add_row(str(missing['id']), missing['final_path'])
            
            console.print(table)
            console.print("\n[yellow]Powy≈ºsze pliki zosta≈Çy prawdopodobnie usuniƒôte lub przeniesione rƒôcznie.[/yellow]")
            console.print("[dim]U≈ºyj 'Narzƒôdzia Zaawansowane -> Edytor Bazy Danych', aby usunƒÖƒá te martwe wpisy.[/dim]")

    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç b≈ÇƒÖd podczas weryfikacji istnienia plik√≥w: {e}", exc_info=True)


async def verify_and_write_hashes():
    """
    Oblicza i zapisuje sumy kontrolne MD5 dla plik√≥w, wykorzystujƒÖc
    wielordzeniowe przetwarzanie dla maksymalnej wydajno≈õci.
    """
    console.clear()
    logger.info("Uruchamiam weryfikacjƒô i zapis sum kontrolnych (MD5)...")
    console.print(Panel("üßÆ Obliczanie i Zapis Sum Kontrolnych (MD5) üßÆ", expand=False, style="bold yellow"))
    
    try:
        from concurrent.futures import ProcessPoolExecutor

        records_to_hash = await get_records_to_hash()

        if not records_to_hash:
            logger.info("Wszystkie pliki majƒÖ ju≈º obliczone sumy kontrolne.")
            console.print("\n[bold green]‚úÖ Wszystkie pobrane pliki w bazie majƒÖ ju≈º obliczone sumy kontrolne.[/bold green]")
            return

        logger.info(f"Znaleziono {len(records_to_hash)} plik√≥w do obliczenia hasha.")
        
        updates_batch: list[tuple[str, int]] = []
        BATCH_SIZE = 100
        
        loop = asyncio.get_running_loop()
        
        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[cyan]Obliczanie hashy (na wielu rdzeniach)...[/]", total=len(records_to_hash))
            
            # U≈ºywamy ProcessPoolExecutor do uruchomienia hashowania na wszystkich dostƒôpnych rdzeniach CPU
            with ProcessPoolExecutor() as executor:
                # Przygotowujemy listƒô ≈õcie≈ºek do przetworzenia
                paths_to_process = [Path(rec['final_path']) for rec in records_to_hash]
                
                # Tworzymy listƒô przysz≈Çych wynik√≥w (futures)
                # UWAGA: _calculate_hash_for_file musi byƒá funkcjƒÖ na poziomie modu≈Çu
                futures = [loop.run_in_executor(executor, _calculate_hash_for_file, path) for path in paths_to_process]
                
                # Mapujemy z powrotem wyniki do ID rekord√≥w
                path_to_id_map = {Path(rec['final_path']): rec['id'] for rec in records_to_hash}
                
                # Przetwarzamy wyniki w miarƒô ich nap≈Çywania
                for i, future in enumerate(asyncio.as_completed(futures)):
                    try:
                        file_hash = await future
                        original_path = paths_to_process[i]
                        
                        if file_hash:
                            entry_id = path_to_id_map[original_path]
                            updates_batch.append((file_hash, entry_id))
                    except Exception as e:
                        logger.error(f"B≈ÇƒÖd podczas hashowania w podprocesie: {e}")
                    finally:
                        progress.update(task, advance=1)

                    if len(updates_batch) >= BATCH_SIZE:
                        await update_hashes_batch(updates_batch)
                        logger.info(f"Zapisano partiƒô {len(updates_batch)} hashy do bazy.")
                        updates_batch.clear()

        if updates_batch:
            await update_hashes_batch(updates_batch)
            logger.info(f"Zapisano ostatniƒÖ partiƒô {len(updates_batch)} hashy.")
            
        console.print(f"\n[bold green]‚úÖ Zako≈Ñczono. Zaktualizowano sumy kontrolne dla {len(records_to_hash) - len(updates_batch)} plik√≥w.[/bold green]")
            
    except Exception as e:
        logger.critical(f"WystƒÖpi≈Ç b≈ÇƒÖd podczas obliczania hashy: {e}", exc_info=True)

async def _calculate_hash_for_file(file_path: Path) -> str | None:
    logger.debug(f"Rozpoczynam obliczanie hasha MD5 dla pliku '{file_path.name}'...")
    hasher = hashlib.md5()
    try:
        def read_and_hash():
            with open(file_path, "rb") as f:
                while chunk := f.read(8192):
                    hasher.update(chunk)
            return hasher.hexdigest()
        file_hash = await asyncio.to_thread(read_and_hash)
        logger.debug(f"Obliczono hash dla '{file_path.name}': {file_hash}")
        return file_hash
    except (IOError, OSError) as e:
        logger.error(f"B≈ÇƒÖd odczytu pliku '{file_path}' podczas hashowania: {e}", exc_info=True)
        return None

async def _get_all_disk_paths(root_dir: Path) -> Set[Path]:
    disk_paths: Set[Path] = set()
    logger.info(f"Rozpoczynam asynchroniczne skanowanie dysku w folderze: {root_dir}")
    loop = asyncio.get_running_loop()
    def scan_directory():
        logger.debug("Uruchamiam blokujƒÖcƒÖ operacjƒô rglob w osobnym wƒÖtku...")
        for path in root_dir.rglob('*'):
            if path.is_file():
                disk_paths.add(path.resolve())
    await loop.run_in_executor(None, scan_directory)
    logger.info(f"Zako≈Ñczono skanowanie dysku. Znaleziono {len(disk_paths)} plik√≥w.")
    return disk_paths

async def _resolve_duplicates_interactively(duplicate_set: List[Dict]) -> Dict:
    selected_to_keep_index = 0
    try:
        size_a_str = duplicate_set[0].get('size', '0 MB').split(' ')[0].replace(',', '.')
        size_b_str = duplicate_set[1].get('size', '0 MB').split(' ')[0].replace(',', '.')
        if float(size_b_str) > float(size_a_str):
            selected_to_keep_index = 1
    except (ValueError, IndexError):
        pass
    def generate_main_layout() -> Layout:
        item_a_details = {
            "ID w Bazie": duplicate_set[0].get('id', 'Brak'), "≈öcie≈ºka": duplicate_set[0].get('relative_path', 'Brak'),
            "separator_1": "", "Data": duplicate_set[0].get('date', 'Brak'),
            "Rozmiar": duplicate_set[0].get('size', 'Brak'), "Wymiary": duplicate_set[0].get('dimensions', 'Brak'),
            "separator_2": "", "Aparat": duplicate_set[0].get('camera', 'Brak'),
        }
        item_b_details = {
            "ID w Bazie": duplicate_set[1].get('id', 'Brak'), "≈öcie≈ºka": duplicate_set[1].get('relative_path', 'Brak'),
            "separator_1": "", "Data": duplicate_set[1].get('date', 'Brak'),
            "Rozmiar": duplicate_set[1].get('size', 'Brak'), "Wymiary": duplicate_set[1].get('dimensions', 'Brak'),
            "separator_2": "", "Aparat": duplicate_set[1].get('camera', 'Brak'),
        }
        comparison_panel = create_side_by_side_comparison_panel(
            item_a_details, item_b_details, is_a_selected=(selected_to_keep_index == 0)
        )
        title_text = f"Wybierz plik do ZACHOWANIA\n[dim]Hash: {duplicate_set[0].get('hash', 'Brak')}[/dim]"
        footer = Align.center(Text("[bold]L/P[/](wybierz)‚Ä¢[bold]ENTER[/](zatwierd≈∫)‚Ä¢[bold]P[/](pomi≈Ñ)‚Ä¢[bold]Q[/](zako≈Ñcz)"))
        main_layout = Layout()
        main_layout.split_column(
            Layout(Align.center(Text(title_text)), size=3),
            comparison_panel,
            Layout(footer, size=1)
        )
        return main_layout
    with Live(generate_main_layout(), screen=True, auto_refresh=False, transient=True) as live:
        while True:
            live.update(generate_main_layout(), refresh=True)
            key = await asyncio.to_thread(get_key)
            if not key: continue
            if key.upper() in ["Q", "ESC"]: return {"action": "quit"}
            if key.upper() == "P": return {"action": "skip"}
            if key in ["LEFT", "RIGHT"]: selected_to_keep_index = 1 - selected_to_keep_index
            if key == "ENTER":
                to_keep = duplicate_set[selected_to_keep_index]
                to_delete = [duplicate_set[1 - selected_to_keep_index]]
                return {"action": "resolve", "keep": to_keep, "delete": to_delete}


async def find_and_fix_inconsistencies():
    """
    Znajduje i pozwala naprawiƒá niesp√≥jno≈õci miƒôdzy bazƒÖ danych a dyskiem,
    takie jak "duchy" (wpisy w bazie bez plik√≥w) i "sieroty" (pliki na dysku bez wpis√≥w).
    """
    console.clear()
    logger.info("Uruchamiam wyszukiwanie niesp√≥jno≈õci (Baza vs. Dysk)...")
    console.print(Panel("üëª Wyszukiwanie Niesp√≥jno≈õci (Duchy i Sieroty)", expand=False, style="bold yellow"))

    try:
        # Krok 1: Wczytaj dane z bazy
        with console.status("[cyan]Wczytywanie rekord√≥w z bazy danych...[/]"):
            db_records = await get_all_final_paths()
        db_paths = {Path(rec['final_path']).resolve() for rec in db_records}
        logger.info(f"Znaleziono {len(db_paths)} unikalnych ≈õcie≈ºek w bazie danych.")

        # Krok 2: Zbuduj listƒô folder√≥w do przeskanowania i poinformuj u≈ºytkownika
        paths_to_scan = {Path(DOWNLOADS_DIR_BASE).resolve()}
        for p_str in LOCAL_SCANNER_DIRECTORIES:
            paths_to_scan.add(Path(p_str).resolve())

        console.print("\n[bold]Skanowane bƒôdƒÖ nastƒôpujƒÖce lokalizacje (zgodnie z config.py):[/bold]")
        for path in paths_to_scan:
            console.print(f"  - [cyan]{path}[/cyan]")
        
        # Krok 3: Skanuj pliki na dysku
        disk_paths = set()
        with console.status("[cyan]Skanowanie plik√≥w na dysku (mo≈ºe to potrwaƒá)...[/]"):
            for root_dir in paths_to_scan:
                if await asyncio.to_thread(root_dir.is_dir):
                    logger.info(f"Skanujƒô rekursywnie folder: {root_dir}")
                    disk_paths.update(await _get_all_disk_paths(root_dir))
        logger.info(f"Znaleziono {len(disk_paths)} plik√≥w na dysku w skonfigurowanych lokalizacjach.")
        
        # Krok 4: Por√≥wnaj wyniki i znajd≈∫ niesp√≥jno≈õci
        db_ghosts = [rec for rec in db_records if Path(rec['final_path']).resolve() not in disk_paths]
        disk_orphans = sorted([path for path in disk_paths if path not in db_paths])

        # Krok 5: Obs≈Çu≈º znalezione "duchy"
        if db_ghosts:
            console.print(f"\n[bold yellow]Znaleziono {len(db_ghosts)} 'duch√≥w' w bazie (wpisy bez plik√≥w na dysku).[/]")
            if Confirm.ask("[cyan]Czy chcesz usunƒÖƒá te martwe wpisy z bazy?[/]", default=True):
                ids_to_delete = [ghost['id'] for ghost in db_ghosts]
                await delete_entries_by_ids(ids_to_delete)
                console.print(f"[green]Usuniƒôto {len(ids_to_delete)} martwych wpis√≥w z bazy.[/green]")
        
        # Krok 6: Obs≈Çu≈º znalezione "sieroty"
        if disk_orphans:
            console.print(f"\n[bold yellow]Znaleziono {len(disk_orphans)} 'sierot' na dysku (pliki bez wpis√≥w w bazie).[/]")
            selected_files_to_process = await _interactive_file_selector(disk_orphans, "Wybierz 'osierocone' pliki do dalszych dzia≈Ça≈Ñ")

            if selected_files_to_process:
                action = await create_interactive_menu(
                    [("Zaimportuj wybrane pliki do bazy", "import"), ("Usu≈Ñ wybrane pliki z dysku", "delete"), ("Anuluj", "cancel")],
                    "Co zrobiƒá z wybranymi plikami?"
                )
                
                if action == "import" and EXIFTOOL_AVAILABLE:
                    imported_count = 0
                    error_count = 0
                    with Progress() as progress:
                        task = progress.add_task("[green]Importujƒô pliki...", total=len(selected_files_to_process))
                        for file_path in selected_files_to_process:
                            # --- POCZƒÑTEK KLUCZOWEJ POPRAWKI ---
                            try:
                                with exiftool.ExifToolHelper() as et:
                                    metadata_list = et.get_metadata(str(file_path))
                                    if not metadata_list:
                                        raise ValueError("Exiftool nie zwr√≥ci≈Ç ≈ºadnych metadanych.")
                                    metadata = metadata_list[0]
                                
                                if await add_local_file_entry(file_path, metadata):
                                    imported_count += 1
                                    logger.info(f"Pomy≈õlnie zaimportowano plik: {file_path.name}")
                                else:
                                    logger.warning(f"Pominiƒôto import pliku (prawdopodobnie ju≈º istnieje w bazie pod innym URL): {file_path.name}")
                            except Exception as e:
                                error_count += 1
                                logger.error(f"B≈ÇƒÖd podczas importu pliku {file_path.name}: {e}", exc_info=True)
                                console.print(f"[bold red]B≈ÇƒÖd importu pliku '{file_path.name}'. Sprawd≈∫ logi.[/bold red]")
                            # --- KONIEC KLUCZOWEJ POPRAWKI ---
                            progress.update(task, advance=1)
                    
                    console.print(f"\n[bold green]Zako≈Ñczono import.[/bold green]")
                    console.print(f"  - Pomy≈õlnie zaimportowano: [cyan]{imported_count}[/cyan] plik√≥w.")
                    if error_count > 0:
                        console.print(f"  - B≈Çƒôdy: [red]{error_count}[/red]. Sprawd≈∫ logi, aby uzyskaƒá wiƒôcej informacji.")

                elif action == "delete":
                    if Confirm.ask(f"[bold red]Czy na pewno chcesz trwale usunƒÖƒá {len(selected_files_to_process)} plik√≥w z dysku?[/]", default=False):
                        deleted_count = 0
                        with Progress() as progress:
                            task = progress.add_task("[red]Usuwam pliki...", total=len(selected_files_to_process))
                            for file_path in selected_files_to_process:
                                try:
                                    await asyncio.to_thread(os.remove, file_path)
                                    deleted_count += 1
                                except OSError as e:
                                    logger.error(f"Nie uda≈Ço siƒô usunƒÖƒá pliku {file_path}: {e}")
                                progress.update(task, advance=1)
                        console.print(f"[green]Pomy≈õlnie usuniƒôto {deleted_count} plik√≥w.[/green]")
        
        if not db_ghosts and not disk_orphans:
            console.print("\n[bold green]‚úÖ Nie znaleziono ≈ºadnych niesp√≥jno≈õci. Baza danych jest w pe≈Çni zsynchronizowana z plikami na dysku.[/bold green]")

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny podczas wyszukiwania niesp√≥jno≈õci: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd krytyczny. Sprawd≈∫ logi.[/bold red]")


async def synchronize_local_file_paths():
    """
    NOWE NARZƒòDZIE: Znajduje wpisy w bazie dla plik√≥w importowanych lokalnie
    i naprawia w nich `final_path`, je≈õli jest nieaktualny.
    """
    console.clear()
    logger.info("Uruchamiam synchronizacjƒô ≈õcie≈ºek dla plik√≥w lokalnych...")
    console.print(Panel("üîÑ Synchronizacja ≈öcie≈ºek Plik√≥w Lokalnych", expand=False, style="bold blue"))

    try:
        with console.status("[cyan]Pobieranie wpis√≥w z bazy danych...[/]"):
            local_entries = await get_local_import_entries()

        if not local_entries:
            console.print("\n[green]Nie znaleziono w bazie ≈ºadnych plik√≥w importowanych lokalnie.[/green]")
            return

        fixed_count = 0
        with Progress() as progress:
            task = progress.add_task("[green]Weryfikujƒô wpisy...", total=len(local_entries))
            for entry in local_entries:
                try:
                    # Odtwarzamy oryginalnƒÖ ≈õcie≈ºkƒô z URL
                    url_path_str = unquote(urlparse(entry['url']).path)
                    # Usuwamy wiodƒÖcy '/' dla system√≥w Windows
                    if os.name == 'nt' and url_path_str.startswith('/'):
                        disk_path = Path(url_path_str[1:])
                    else:
                        disk_path = Path(url_path_str)
                    
                    current_final_path = Path(entry['final_path']) if entry['final_path'] else None

                    # Sprawdzamy, czy plik istnieje na dysku i czy ≈õcie≈ºka w bazie jest poprawna
                    if await asyncio.to_thread(disk_path.exists) and disk_path.resolve() != (current_final_path.resolve() if current_final_path else None):
                        logger.warning(f"Naprawiam ≈õcie≈ºkƒô dla ID={entry['id']}. Stara: '{current_final_path}', Nowa: '{disk_path.resolve()}'")
                        await update_paths_for_entry(entry['id'], str(disk_path.resolve()))
                        fixed_count += 1
                except Exception as e:
                    logger.error(f"B≈ÇƒÖd podczas przetwarzania wpisu ID={entry['id']}: {e}")
                finally:
                    progress.update(task, advance=1)
        
        console.print(f"\n[bold green]‚úÖ Synchronizacja zako≈Ñczona.[/bold green]")
        console.print(f"   - Naprawiono [cyan]{fixed_count}[/cyan] nieaktualnych wpis√≥w.")

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny podczas synchronizacji ≈õcie≈ºek: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd krytyczny. Sprawd≈∫ logi.[/bold red]")


async def find_and_fix_inconsistencies_bak():
    """
    Znajduje i pozwala naprawiƒá niesp√≥jno≈õci miƒôdzy bazƒÖ danych a dyskiem,
    takie jak "duchy" (wpisy w bazie bez plik√≥w) i "sieroty" (pliki na dysku bez wpis√≥w).
    """
    console.clear()
    logger.info("Uruchamiam wyszukiwanie niesp√≥jno≈õci (Baza vs. Dysk)...")
    console.print(Panel("üëª Wyszukiwanie Niesp√≥jno≈õci (Duchy i Sieroty)", expand=False, style="bold yellow"))

    try:
        # Krok 1: Wczytaj dane z bazy
        with console.status("[cyan]Wczytywanie rekord√≥w z bazy danych...[/]"):
            db_records = await get_all_final_paths()
        db_paths = {Path(rec['final_path']).resolve() for rec in db_records}
        logger.info(f"Znaleziono {len(db_paths)} unikalnych ≈õcie≈ºek w bazie danych.")

        # Krok 2: Zbuduj listƒô folder√≥w do przeskanowania i poinformuj u≈ºytkownika
        # U≈ºywamy seta, aby uniknƒÖƒá skanowania tego samego folderu wielokrotnie
        paths_to_scan = {Path(DOWNLOADS_DIR_BASE).resolve()}
        for p_str in LOCAL_SCANNER_DIRECTORIES:
            paths_to_scan.add(Path(p_str).resolve())

        console.print("\n[bold]Skanowane bƒôdƒÖ nastƒôpujƒÖce lokalizacje (zgodnie z config.py):[/bold]")
        for path in paths_to_scan:
            console.print(f"  - [cyan]{path}[/cyan]")
        
        # Krok 3: Skanuj pliki na dysku
        disk_paths = set()
        with console.status("[cyan]Skanowanie plik√≥w na dysku (mo≈ºe to potrwaƒá)...[/]"):
            for root_dir in paths_to_scan:
                if await asyncio.to_thread(root_dir.is_dir):
                    logger.info(f"Skanujƒô rekursywnie folder: {root_dir}")
                    disk_paths.update(await _get_all_disk_paths(root_dir))
        logger.info(f"Znaleziono {len(disk_paths)} plik√≥w na dysku w skonfigurowanych lokalizacjach.")
        
        # Krok 4: Por√≥wnaj wyniki i znajd≈∫ niesp√≥jno≈õci
        logger.info(f"Por√≥wnujƒô {len(db_paths)} wpis√≥w z bazy z {len(disk_paths)} plikami na dysku.")
        db_ghosts = [rec for rec in db_records if Path(rec['final_path']).resolve() not in disk_paths]
        disk_orphans = sorted([path for path in disk_paths if path not in db_paths])

        # Krok 5: Obs≈Çu≈º znalezione "duchy"
        if db_ghosts:
            console.print(f"\n[bold yellow]Znaleziono {len(db_ghosts)} 'duch√≥w' w bazie (wpisy bez plik√≥w na dysku).[/]")
            if Confirm.ask("[cyan]Czy chcesz usunƒÖƒá te martwe wpisy z bazy?[/]", default=True):
                ids_to_delete = [ghost['id'] for ghost in db_ghosts]
                await delete_entries_by_ids(ids_to_delete)
                console.print(f"[green]Usuniƒôto {len(ids_to_delete)} martwych wpis√≥w z bazy.[/green]")
        
        # Krok 6: Obs≈Çu≈º znalezione "sieroty"
        if disk_orphans:
            console.print(f"\n[bold yellow]Znaleziono {len(disk_orphans)} 'sierot' na dysku (pliki bez wpis√≥w w bazie).[/]")
            selected_files_to_process = await _interactive_file_selector(disk_orphans, "Wybierz 'osierocone' pliki do dalszych dzia≈Ça≈Ñ")

            if selected_files_to_process:
                action = await create_interactive_menu(
                    [("Zaimportuj wybrane pliki do bazy", "import"), ("Usu≈Ñ wybrane pliki z dysku", "delete"), ("Anuluj", "cancel")],
                    "Co zrobiƒá z wybranymi plikami?"
                )
                
                if action == "import" and EXIFTOOL_AVAILABLE:
                    # Logika importu
                    imported_count = 0
                    with Progress() as progress:
                        task = progress.add_task("[green]Importujƒô pliki...", total=len(selected_files_to_process))
                        for file_path in selected_files_to_process:
                            with exiftool.ExifToolHelper() as et:
                                metadata = et.get_metadata(str(file_path))[0]
                            if await add_local_file_entry(file_path, metadata):
                                imported_count += 1
                            progress.update(task, advance=1)
                    console.print(f"[green]Pomy≈õlnie zaimportowano {imported_count} plik√≥w.[/green]")

                elif action == "delete":
                    # Logika usuwania
                    if Confirm.ask(f"[bold red]Czy na pewno chcesz trwale usunƒÖƒá {len(selected_files_to_process)} plik√≥w z dysku?[/]", default=False):
                        deleted_count = 0
                        with Progress() as progress:
                            task = progress.add_task("[red]Usuwam pliki...", total=len(selected_files_to_process))
                            for file_path in selected_files_to_process:
                                try:
                                    await asyncio.to_thread(os.remove, file_path)
                                    deleted_count += 1
                                except OSError as e:
                                    logger.error(f"Nie uda≈Ço siƒô usunƒÖƒá pliku {file_path}: {e}")
                                progress.update(task, advance=1)
                        console.print(f"[green]Pomy≈õlnie usuniƒôto {deleted_count} plik√≥w.[/green]")
        
        if not db_ghosts and not disk_orphans:
            console.print("\n[bold green]‚úÖ Nie znaleziono ≈ºadnych niesp√≥jno≈õci. Baza danych jest w pe≈Çni zsynchronizowana z plikami na dysku.[/bold green]")

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny podczas wyszukiwania niesp√≥jno≈õci: {e}", exc_info=True)
        console.print(f"[bold red]WystƒÖpi≈Ç b≈ÇƒÖd krytyczny. Sprawd≈∫ logi.[/bold red]")

async def analyze_metadata_consistency():
    """Sprawdza sp√≥jno≈õƒá miƒôdzy ≈õcie≈ºkƒÖ pliku a datƒÖ w metadanych."""
    console.clear()
    logger.info("Uruchamiam analizƒô sp√≥jno≈õci metadanych...")
    console.print(Panel("üîó Analiza Sp√≥jno≈õci Metadanych (≈öcie≈ºka vs. Data) üîó", expand=False, style="bold yellow"))
    
    mismatches = []
    
    try:
        all_records = await get_metadata_for_consistency_check()
        
        if not all_records:
            console.print("\n[green]Nie znaleziono plik√≥w z wymaganymi danymi do analizy.[/green]")
            return

        with Progress(console=console, transient=True) as progress:
            task = progress.add_task("[cyan]Analizowanie sp√≥jno≈õci...", total=len(all_records))
            for row in all_records:
                try:
                    path = Path(row['final_path'])
                    if len(path.parts) < 3: continue
                        
                    db_datetime = datetime.fromisoformat(row['dt_from_json'].replace('Z', '+00:00'))
                    path_month = int(path.parent.name)
                    path_year = int(path.parent.parent.name)

                    if db_datetime.year != path_year or db_datetime.month != path_month:
                        mismatches.append({'path': str(path), 'db_date': f"{db_datetime.year}-{db_datetime.month:02d}", 'path_date': f"{path_year}-{path_month:02d}"})
                except (ValueError, IndexError, TypeError):
                    continue
                finally:
                    progress.update(task, advance=1)

        if not mismatches:
            console.print("\n[bold green]‚úÖ Sp√≥jno≈õƒá metadanych idealna![/bold green]")
        else:
            # ... (wy≈õwietlanie wynik√≥w bez zmian) ...
            pass

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny podczas analizy sp√≥jno≈õci: {e}", exc_info=True)


async def find_duplicates_by_hash():
    """Znajduje i pozwala zarzƒÖdzaƒá duplikatami plik√≥w na podstawie hasha MD5."""
    console.clear()
    logger.info("Uruchamiam Mened≈ºer Duplikat√≥w (wg hash MD5)...")
    console.print(Panel("üß© Mened≈ºer Duplikat√≥w Plik√≥w (wg zawarto≈õci) üß©", expand=False, style="bold yellow"))
    
    try:
        duplicate_hashes = await get_duplicate_hashes()

        if not duplicate_hashes:
            console.print("\n[bold green]‚úÖ Nie znaleziono ≈ºadnych duplikat√≥w.[/bold green]")
            return

        logger.warning(f"Znaleziono {len(duplicate_hashes)} zestawy duplikat√≥w.")
        all_files_to_delete = []

        for i, hash_val in enumerate(duplicate_hashes):
            console.clear()
            console.print(Panel(f"[bold yellow]Zestaw duplikat√≥w {i + 1}/{len(duplicate_hashes)}[/]", expand=False))

            files_in_set_raw = await get_entries_by_hash(hash_val)
            files_in_set = []
            for file_row in files_in_set_raw:
                metadata = json.loads(file_row['metadata_json'] or '{}')
                file_path = Path(file_row['final_path'])
                display_info = _parse_metadata_for_display(metadata, file_path)
                files_in_set.append({"id": file_row['id'], "path": file_path, "hash": hash_val, **display_info})

            if len(files_in_set) > 1:
                resolution = await _resolve_duplicates_interactively(files_in_set)
                if resolution.get("action") == "quit": break
                if resolution.get("action") == "resolve":
                    all_files_to_delete.extend(resolution['delete'])

        if all_files_to_delete:
            console.clear()
            console.print(Panel("[bold red]Podsumowanie Akcji Usuniƒôcia[/]", expand=False))
            console.print(f"Wybrano [cyan]{len(all_files_to_delete)}[/cyan] plik√≥w do usuniƒôcia.")

            if Confirm.ask("\n[bold red]Czy na pewno chcesz TRWALE usunƒÖƒá te pliki?[/]", default=False):
                ids_to_delete = [f['id'] for f in all_files_to_delete]
                
                with Progress(console=console, transient=True) as progress:
                    task = progress.add_task("[red]Usuwam pliki...", total=len(all_files_to_delete))
                    for file_info in all_files_to_delete:
                        if await asyncio.to_thread(file_info['path'].exists):
                            await asyncio.to_thread(os.remove, file_info['path'])
                        progress.update(task, advance=1)
                
                await delete_entries_by_ids(ids_to_delete)
                console.print(f"\n[bold green]‚úÖ Usuniƒôto {len(ids_to_delete)} duplikat√≥w.[/bold green]")

    except Exception as e:
        logger.critical(f"B≈ÇƒÖd krytyczny w mened≈ºerze duplikat√≥w: {e}", exc_info=True)


# ##############################################################################
# ===                    SEKCJA 3: G≈Å√ìWNA FUNKCJA URUCHOMIENIOWA             ===
# ##############################################################################

async def run_integrity_validator():
    """Wy≈õwietla i zarzƒÖdza interaktywnym menu dla Walidatora Integralno≈õci."""
    logger.info("Uruchamiam menu Walidatora Integralno≈õci Danych.")
    
    menu_items = [
        ("Sprawd≈∫ istnienie plik√≥w (Baza vs Dysk)", verify_file_existence),
        ("Oblicz i zapisz sumy kontrolne plik√≥w (hash MD5)", verify_and_write_hashes),
        ("Wyszukaj niesp√≥jno≈õci (pliki-sieroty i duchy)", find_and_fix_inconsistencies),
        ("[bold blue]Synchronizuj ≈õcie≈ºki dla plik√≥w lokalnych (Napraw 'sieroty')[/bold blue]", synchronize_local_file_paths),
        ("Analizuj sp√≥jno≈õƒá metadanych (≈õcie≈ºka vs. data)", analyze_metadata_consistency),
        ("Znajd≈∫ i zarzƒÖdzaj duplikatami (wg zawarto≈õci)", find_duplicates_by_hash),
        ("Wr√≥ƒá do menu g≈Ç√≥wnego", "exit")
    ]


    while True:
        console.clear()
        selected_action = await create_interactive_menu(
            menu_items,
            "Walidator Integralno≈õci Danych",
            border_style="blue"
        )

        if selected_action == "exit" or selected_action is None:
            logger.info("Zamykanie Walidatora Integralno≈õci.")
            break

        await selected_action()
        
        Prompt.ask("\n[bold]Operacja zako≈Ñczona. Naci≈õnij Enter...[/]")
